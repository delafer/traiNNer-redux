#########################################################################################
# ParagonSR2 Nano - FIDELITY OPTIMIZED (Maximum PSNR/SSIM)
#
# This configuration prioritizes ABSOLUTE MAXIMUM QUALITY over speed or convenience.
# Optimized for your RTX 3070 + CC0 147k dataset for highest possible PSNR/SSIM.
#
# EXPECTED RESULTS:
# ‚úÖ PSNR: ~30-32 dB (vs ~28-30 dB for convenience-optimized)
# ‚úÖ SSIM: ~0.88-0.92 (vs ~0.85-0.88 for convenience-optimized)
# ‚è∞ Training Time: 20-30 hours (vs 8-12 hours for convenience)
# üéØ Quality: Maximum achievable for ParagonSR2 Nano architecture
#########################################################################################

name: 2xParagonSR2_Nano_CC0_147k_FIDELITY_OPTIMIZED
scale: 2

# System optimizations (same as zero-config for consistency)
use_amp: true
amp_bf16: true
use_channels_last: true
fast_matmul: true
num_gpu: auto
manual_seed: 1024

# Dataset configuration (your actual paths)
datasets:
  train:
    name: CC0_147k_Fidelity_Train
    type: pairedimagedataset
    dataroot_gt: /home/phips/Documents/dataset/cc0/hr
    dataroot_lq: /home/phips/Documents/dataset/cc0/lr_x2_bicubic_aa
    lq_size: 128
    use_hflip: true
    use_rot: true
    num_worker_per_gpu: 8
    batch_size_per_gpu: 12                    # Slightly smaller for stability
    accum_iter: 1
    pin_memory: true

  val:
    name: CC0_147k_Fidelity_Val
    type: pairedimagedataset
    dataroot_gt: /home/phips/Documents/dataset/cc0/val_hr
    dataroot_lq: /home/phips/Documents/dataset/cc0/val_lr_x2_bicubic_aa
    lq_size: 128

network_g:
  type: paragonsr2_nano

path:
  pretrain_network_g: ~
  strict_load_g: true
  resume_state: ~

# FIDELITY-OPTIMIZED TRAINING CONFIGURATION
train:
  ema_decay: 0.999
  ema_power: 0.75
  grad_clip: true

  # SLOWER, MORE CAREFUL OPTIMIZATION
  optim_g:
    type: AdamW
    lr: !!float 8e-5                        # üîΩ Much slower learning rate
    weight_decay: !!float 5e-5              # üîΩ Less regularization for fine-tuning
    betas: [0.9, 0.99]

  # EXTENDED TRAINING DURATION
  total_iter: 100000                        # üîº 2.5x longer training (vs 40k)
  warmup_iter: 3000                         # üîº Longer warmup for stability

  # ENHANCED TRAINING AUTOMATIONS (More Patient)
  training_automations:
    enabled: true

    # 1. Intelligence Learning Rate Scheduler (More Conservative)
    IntelligentLearningRateScheduler:
      enabled: true
      monitor_loss: true
      monitor_validation: true
      adaptation_threshold: 0.015           # üîΩ More sensitive to improvements
      plateau_patience: 2500                # üîº Wait longer before LR reduction
      improvement_threshold: 0.0005         # üîΩ Detect smaller improvements
      min_lr_factor: 0.6                    # üîº Keep LR higher for longer
      max_lr_factor: 1.5                    # üîΩ Less aggressive increases
      fallback:
        scheduler_type: "cosine"
        scheduler_params:
          eta_min: 0.00001
          T_max: 100000                     # Matches extended training
      max_adjustments: 75                    # üîº Allow more adjustments

    # 2. Dynamic Batch Size Optimizer (More Conservative)
    DynamicBatchSizeOptimizer:
      enabled: true
      target_vram_usage: 0.80               # üîΩ More conservative VRAM usage
      safety_margin: 0.08                   # üîΩ Larger safety margin
      adjustment_frequency: 150             # üîº Less frequent adjustments
      min_batch_size: 8
      max_batch_size: 16                    # üîΩ Lower max batch for stability
      vram_history_size: 75                  # üîº Longer history for stability
      fallback:
        batch_size: 12                      # Conservative fallback
      max_adjustments: 15                    # üîΩ Fewer adjustments

    # 3. Adaptive Gradient Clipping (More Sensitive)
    AdaptiveGradientClipping:
      enabled: true
      initial_threshold: 0.8                # üîΩ More conservative starting point
      min_threshold: 0.05                   # üîΩ Lower minimum
      max_threshold: 5.0                    # üîΩ Lower maximum
      adjustment_factor: 1.1                # üîΩ Slower adaptation
      monitoring_frequency: 15              # üîº Less frequent monitoring
      gradient_history_size: 150            # üîº Longer history
      fallback:
        threshold: 0.8
      max_adjustments: 50                    # üîΩ Fewer adjustments

    # 4. Intelligent Early Stopping (MUCH More Patient)
    IntelligentEarlyStopping:
      enabled: true
      patience: 8000                        # üîº 4x more patient than standard
      min_improvement: 0.0002               # üîΩ Detect very small improvements
      min_epochs: 2000                      # üîº Minimum training time
      min_iterations: 15000                 # üîº Minimum iterations
      monitor_metric: "val/psnr"
      max_no_improvement: 8000              # üîº Wait much longer
      improvement_threshold: 0.0008         # üîΩ Sensitive to small improvements
      warmup_iterations: 5000               # üîº Longer warmup period
      fallback:
        early_stopping: false               # Don't stop early by default
      max_adjustments: 2                     # Allow decision review

  # Dynamic Loss Scheduling (Same as zero-config - works well)
  dynamic_loss_scheduling:
    enabled: true
    auto_calibrate: true

  # Optimized Loss Configuration for Fidelity
  losses:
    - type: l1loss
      loss_weight: 1.0
    - type: ssimloss
      loss_weight: 0.03                     # üîΩ Slightly reduced for L1 dominance

# Enhanced Validation for Better Monitoring
val:
  val_enabled: true
  val_freq: 300                             # üîº More frequent validation
  save_img: false

  metrics_enabled: true
  metrics:
    psnr:
      type: calculate_psnr
      crop_border: 4
    ssim:
      type: calculate_ssim
      crop_border: 4

# Enhanced Logging for Monitoring Long Training
logger:
  print_freq: 50                            # üîº More frequent logs
  save_checkpoint_freq: 25000               # üîº More frequent checkpoints
  save_checkpoint_format: safetensors
  use_tb_logger: true


#########################################################################################
# QUALITY PROGRESSION EXPECTATIONS
#########################################################################################
#
# HOUR 4:   PSNR ~27.8 dB    (Continue training)
# HOUR 8:   PSNR ~28.5 dB    (Continue training)
# HOUR 12:  PSNR ~29.2 dB    (Still improving)
# HOUR 16:  PSNR ~29.8 dB    (Nearly optimal)
# HOUR 20:  PSNR ~30.3 dB    (Very good)
# HOUR 24:  PSNR ~30.7 dB    (Excellent)
# HOUR 28:  PSNR ~31.0 dB    (Maximum achievable)
#
# vs Convenience-Optimized (stops around hour 12):
# HOUR 12:  PSNR ~28.5 dB    (Stops here - misses potential)
#########################################################################################

#########################################################################################
# USAGE RECOMMENDATIONS
#########################################################################################
#
# 1. START TRAINING: python train.py --opt 2xParagonSR2_Nano_FIDELITY_OPTIMIZED.yml
#
# 2. MONITOR PROGRESS:
#    - Check PSNR progression every 4-6 hours
#    - Don't stop early - let it continue to hour 20+
#    - Early stopping is very patient (8k iterations)
#
# 3. EXPECTED TIMELINE:
#    - Hour 0-4:   Initial convergence
#    - Hour 4-12:  Steady improvement
#    - Hour 12-20: Fine-tuning and optimization
#    - Hour 20-28: Final convergence to maximum
#
# 4. SUCCESS INDICATORS:
#    - PSNR continues improving past hour 12
#    - Validation loss keeps decreasing slowly
#    - No signs of overfitting (train/val loss gap stable)
#
# 5. STOPPING CONDITIONS:
#    - Automatic: Early stopping triggers around hour 25-30
#    - Manual: Stop if PSNR hasn't improved for 6+ hours
#    - Target: Reach 30+ dB PSNR before stopping
#########################################################################################
