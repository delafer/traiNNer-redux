# yaml-language-server: $schema=https://raw.githubusercontent.com/the-database/traiNNer-redux/refs/heads/master/schemas/redux-config.schema.json
# 2x ParagonSR2 Nano - Clean Fidelity Pretrain
#
# Goals:
# - Train a clean, sharp, artifact-minimized 2x backbone for ParagonSR2 Nano.
# - Use the updated ParagonSR2 architecture:
#     * LR backbone
#     * MagicKernelSharp2021 upsampler with upsampler_alpha=0.5
#     * 1x HR ResidualBlock refinement head (hr_blocks=1)
# - No GAN, no perceptual loss here: this is a stable, deployment-ready base.
# - Loss: strong L1 + very light frequency regularization to suppress ringing.
#
# This checkpoint is intended as pretrain_network_g for the perceptual config.

name: 2x_ParagonSR2_Nano_Pretrain_Clean
scale: 2

use_amp: true
amp_bf16: false
use_channels_last: true
fast_matmul: true
num_gpu: auto

# -------------------------------------------------------------------------------------------------
# Data
# -------------------------------------------------------------------------------------------------
datasets:
  train:
    name: Train_Dataset
    type: pairedimagedataset
    dataroot_gt: /home/phips/Documents/dataset/cc0/hr
    dataroot_lq: /home/phips/Documents/dataset/cc0/lr_x2
    lq_size: 256
    use_hflip: true
    use_rot: true
    num_worker_per_gpu: 8
    batch_size_per_gpu: 16
    accum_iter: 1

  val:
    name: Val_Dataset
    type: pairedimagedataset
    dataroot_gt: /home/phips/Documents/dataset/cc0/val_hr
    dataroot_lq: /home/phips/Documents/dataset/cc0/val_lr_x2

# -------------------------------------------------------------------------------------------------
# Network
# -------------------------------------------------------------------------------------------------
network_g:
  type: paragonsr2_nano
  # Explicit for clarity; these match the ParagonSR2 Nano defaults:
  upsampler_alpha: 0.5   # sharp but controlled Magic upsampler
  hr_blocks: 1           # tiny HR refinement head (single ResidualBlock)

# -------------------------------------------------------------------------------------------------
# Paths
# -------------------------------------------------------------------------------------------------
path:
  pretrain_network_g: ~     # Training from scratch
  strict_load_g: true
  resume_state: ~

# -------------------------------------------------------------------------------------------------
# Training
# -------------------------------------------------------------------------------------------------
train:
  ema_decay: 0.995          # Light EMA for stability; does not change architecture behavior.
  ema_power: 0.75
  grad_clip: true

  optim_g:
    type: AdamW
    lr: !!float 1.5e-4
    weight_decay: 0.0
    betas: [0.9, 0.99]

  scheduler:
    type: CosineAnnealingLR
    T_max: 200000
    eta_min: !!float 1e-7

  total_iter: 200000
  warmup_iter: 2000

  # Clean, artifact-aware fidelity loss:
  losses:
    - type: l1loss
      loss_weight: 1.0

    # Gentle frequency regularization:
    # Encourages matching HR frequency structure without encouraging oscillatory ringing.
    - type: ffloss
      loss_weight: 0.05

    # If you implement and register a tvloss (TV regularizer) in traiNNer:
    # you may enable this with a very small weight (0.005â€“0.01) to further
    # discourage tiny oscillations. Keep it commented out otherwise.
    # - type: tvloss
    #   loss_weight: 0.01

# -------------------------------------------------------------------------------------------------
# Validation
# -------------------------------------------------------------------------------------------------
val:
  val_enabled: true
  val_freq: 1000
  save_img: false

  metrics_enabled: true
  metrics:
    psnr:
      type: calculate_psnr
      crop_border: 2
      test_y_channel: false
    ssim:
      type: calculate_ssim
      crop_border: 2
      test_y_channel: false

# -------------------------------------------------------------------------------------------------
# Logging
# -------------------------------------------------------------------------------------------------
logger:
  print_freq: 100
  save_checkpoint_freq: 5000
  save_checkpoint_format: safetensors
  use_tb_logger: true
