#########################################################################################
# ParagonSR2 S Fidelity Config
# Optimized for highest PSNR scores with intelligent training automations
# Enhanced: Dynamic loss scheduling + Auto-calibration + Training automations + L1+MS-SSIM
# Architecture: ParagonSR2_S (larger model requiring different hyperparameters)
#########################################################################################
name: 2xParagonSR2_S_CC0_complexity05_ULTRA_FIDELITY
scale: 2

use_amp: true
amp_bf16: true
use_channels_last: true
fast_matmul: false  # Precision over speed for larger model
num_gpu: auto
#manual_seed: 1024

datasets:
  train:
    name: CC0_147k_Train
    type: pairedimagedataset
    dataroot_gt: /home/phips/Documents/dataset/cc0/filtered_versions/cc0_c5_hr
    dataroot_lq: /home/phips/Documents/dataset/cc0/filtered_versions/cc0_c5_lr_x2
    lq_size: 128
    use_hflip: true
    use_rot: true
    num_worker_per_gpu: 8
    batch_size_per_gpu: 12  # Smaller batch size due to larger model memory usage
    accum_iter: 1

  val:
    name: CC0_147k_Val
    type: pairedimagedataset
    dataroot_gt: /home/phips/Documents/dataset/cc0/val_hr
    dataroot_lq: /home/phips/Documents/dataset/cc0/val_lr_x2_bicubic_aa

network_g:
  type: paragonsr2_s
  # S architecture uses larger feature dimensions and more blocks
  # Optimized for highest quality at the cost of training speed

path:
  pretrain_network_g: ~
  strict_load_g: true
  resume_state: ~

train:
  ema_decay: 0.999
  ema_power: 0.75
  grad_clip: true

  optim_g:
    type: AdamW
    lr: !!float 1e-4  # Lower LR for larger, more complex model stability
    weight_decay: !!float 1e-4
    betas: [0.9, 0.99]

  # Extended training for maximum convergence on large model + high-quality dataset
  scheduler:
    type: MultiStepLR
    milestones: [60000, 80000, 95000]  # Extended milestones for 100k training
    gamma: 0.5

  total_iter: 100000  # Maximum training for highest PSNR potential on large model
  warmup_iter: 1500   # Longer warmup for stability with larger model

  # DYNAMIC LOSS SCHEDULING WITH AUTO-CALIBRATION
  # Auto-calibration analyzes dataset complexity to set optimal starting weights
  dynamic_loss_scheduling:
    enabled: true                    # Enable intelligent loss adaptation
    momentum: 0.9                    # Smooth adaptation (0.0-1.0)
    adaptation_rate: 0.008           # Slower adaptation for larger model stability
    min_weight: 1e-6                 # Minimum possible weight
    max_weight: 100.0                # Maximum possible weight
    adaptation_threshold: 0.03       # More conservative adaptation for stability
    baseline_iterations: 300         # Establish baseline before adapting (longer for S)
    enable_monitoring: true          # Detailed logging
    auto_calibrate: true             # Auto-detect dataset complexity for optimal starting weights

  # TRAINING AUTOMATIONS - Intelligent hyperparameter optimization
  training_automations:
    intelligent_learning_rate_scheduler:
      enabled: true                   # Auto-adapt LR based on convergence progress
      strategy: "adaptive"            # adaptive, cosine, exponential, plateau
      adaptation_frequency: 1500      # Check every 1500 iterations (less frequent for S)
      improvement_threshold: 0.0008   # Slightly lower threshold for larger model

    dynamic_batch_size_optimizer:
      enabled: true                   # Auto-optimize batch size for VRAM efficiency
      target_vram_usage: 0.80         # Use 80% of available VRAM (more conservative for S)
      safety_margin: 0.08            # Keep 8% free for stability
      adjustment_frequency: 1000      # Check every 1000 iterations

    early_stopping:
      enabled: true                   # Auto-stop when converged
      patience: 5000                  # Wait 5000 iterations without improvement (longer for S)
      min_improvement: 0.0003         # Lower improvement threshold for larger model
      metric: "val/psnr"              # Monitor validation PSNR
      save_best: true                 # Save best performing model

  # OPTIMAL FIDELITY LOSS COMBINATION
  # L1 for pixel accuracy + MS-SSIM for structural similarity
  losses:
    - type: l1loss
      loss_weight: 1.0                # Primary loss for pixel-accurate reconstruction
    - type: ssimloss
      loss_weight: 0.08               # Slightly higher weight for S model's perceptual capabilities

val:
  val_enabled: true
  val_freq: 1000                      # More frequent validation for monitoring
  save_img: true                      # Save validation images for quality inspection

  metrics_enabled: true
  metrics:
    psnr:
      type: calculate_psnr
      crop_border: 4
    ssim:
      type: calculate_ssim
      crop_border: 4

logger:
  print_freq: 100
  save_checkpoint_freq: 10000         # Save more frequently for longer training
  save_checkpoint_format: safetensors
  use_tb_logger: true
