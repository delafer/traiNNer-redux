# yaml-language-server: $schema=https://raw.githubusercontent.com/the-database/traiNNer-redux/refs/heads/master/schemas/redux-config.schema.json
# Stage 2B: ParagonSR2 Nano 2x Perceptual - Sharpened finetune
# - Start from best Stage 2A (conservative perceptual) EMA
# - Stronger but controlled perceptual + R3GAN + freq
# - Goal: crisp, clean, visually pleasing outputs (no GAN watercolor)

name: 2x_ParagonSR2_Nano_Perceptual_Stage2B
scale: 2

use_amp: true
amp_bf16: false
use_channels_last: true
fast_matmul: true
num_gpu: auto

# -------------------------------------------------------------------------------------------------
# Data
# -------------------------------------------------------------------------------------------------
datasets:
  train:
    name: Train_Dataset
    type: pairedimagedataset
    dataroot_gt: /home/phips/Documents/dataset/cc0/hr
    dataroot_lq: /home/phips/Documents/dataset/cc0/lr_x2
    lq_size: 64
    use_hflip: true
    use_rot: true
    num_worker_per_gpu: 8
    batch_size_per_gpu: 8
    accum_iter: 1

  val:
    name: Val_Dataset
    type: pairedimagedataset
    dataroot_gt: /home/phips/Documents/dataset/cc0/val_hr
    dataroot_lq: /home/phips/Documents/dataset/cc0/val_lr_x2

# -------------------------------------------------------------------------------------------------
# Network
# -------------------------------------------------------------------------------------------------
network_g:
  type: paragonsr2_nano

network_d:
  type: munet

# -------------------------------------------------------------------------------------------------
# Paths
# -------------------------------------------------------------------------------------------------
path:
  # Use EMA from Stage 2A:
  pretrain_network_g: experiments/2x_ParagonSR2_Nano_Perceptual_Stage2/models/net_g_ema_70000.safetensors
  strict_load_g: true
  resume_state: ~

# -------------------------------------------------------------------------------------------------
# Training
# -------------------------------------------------------------------------------------------------
train:
  ema_decay: 0.999
  grad_clip: true

  optim_g:
    type: AdamW
    lr: 1.5e-4
    weight_decay: 0.0
    betas: [0.9, 0.99]

  optim_d:
    type: AdamW
    lr: 1.5e-4
    weight_decay: 0.0
    betas: [0.9, 0.99]

  # Shorter, focused finetune schedule
  scheduler:
    type: CosineAnnealingLR
    T_max: 40000
    eta_min: 1.0e-7

  total_iter: 40000
  warmup_iter: 1000

  # ---
  # Losses: more decisive perceptual + sharpness, still constrained.
  # ---
  losses:
    # 1) Fidelity (reduced but present)
    - type: l1loss
      loss_weight: 0.8

    - type: mssimloss
      loss_weight: 0.15

    # 2) Modern perceptual
    - type: convnextperceptualloss
      loss_weight: 0.25

    - type: distsloss
      loss_weight: 0.30

    # 3) Contrastive (softer)
    - type: ContrastiveLoss
      loss_weight: 0.05
      temperature: 0.1

    # 4) Consistency (neutral, lighter)
    - type: ConsistencyLoss
      loss_weight: 0.12
      criterion: chc
      blur: false
      saturation: 1.0
      brightness: 1.0
      cosim: true
      cosim_weight: 0.5

    # 5) Frequency sharpening (stronger)
    - type: ffloss
      loss_weight: 0.35

    # 6) Adversarial (R3GAN) - stronger but still controlled
    - type: ganloss
      gan_type: r3gan
      loss_weight: 0.05
      r1_weight: 2.5
      r2_weight: 2.5

# -------------------------------------------------------------------------------------------------
# Validation
# -------------------------------------------------------------------------------------------------
val:
  val_enabled: true
  val_freq: 5000
  save_img: true

  metrics_enabled: true
  metrics:
    psnr:
      type: calculate_psnr
      crop_border: 2
      test_y_channel: false
    ssim:
      type: calculate_ssim
      crop_border: 2
      test_y_channel: false
    lpips:
      type: calculate_lpips
      better: lower
    dists:
      type: calculate_dists
      better: lower
    topiq:
      type: calculate_topiq

# -------------------------------------------------------------------------------------------------
# Logging
# -------------------------------------------------------------------------------------------------
logger:
  print_freq: 100
  save_checkpoint_freq: 5000
  save_checkpoint_format: safetensors
  use_tb_logger: true
