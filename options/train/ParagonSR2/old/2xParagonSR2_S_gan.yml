# yaml-language-server: $schema=https://raw.githubusercontent.com/the-database/traiNNer-redux/refs/heads/master/schemas/redux-config.schema.json
# -------------------------------------------------------------------------------------------------
# V2 CONFIGURATION RATIONALE: ARTIFACT-FREE & CRISP (ParagonSR2 Nano Specific)
# -------------------------------------------------------------------------------------------------
#
# **Core Philosophy vs. Default Configs (e.g., HAT_M_gan.yml):**
#
# Default traiNNer-redux GAN configurations are designed for maximal perceptual scores, often
# accepting a higher risk of artifacts (oil-paint texture, halos) inherent to strong adversarial
# training.
#
# This **V2 "Clean Perceptual"** configuration is a targeted, modern recipe focused exclusively on
# **artifact-free, clean, crisp, and visually compelling** outputs suitable for production deployment.
# It achieves this by:
# 1.  Using fewer, more modern, and less correlated loss components (DINO + LDL).
# 2.  Employing aggressive artifact suppression techniques (LDL, GV, low GAN weight).
# 3.  Tuning optimizers for stability and a strong adversarial signal (TTUR, WD on G).
#
# The primary distinction is the shift from a 'score-chasing' approach to a 'human-preference, clean-output' approach.

# -------------------------------------------------------------------------------------------------
# PERFORMANCE OPTIMIZATION STRATEGY: FAST TRAINING → QUALITY FINE-TUNING
# -------------------------------------------------------------------------------------------------
#
# **Two-Phase Training Approach (RECOMMENDED for fastest training with good quality):**
#
# **PHASE 1 - Fast Training (0-150k iterations):**
#   dynamic_training_mode: "cheap"    # Fast SE-like channel attention vs. expensive per-sample kernels
#   fast_body_mode: true              # Half the depth (3→1.5 groups, 4→2 blocks) for faster training
#   use_channels_last: true           # Already enabled - GPU memory optimization
#   dynamic_update_every: 8           # Update tracked kernels every 8 batches vs. every 1
#   → EXPECTED: 1.8-3× faster training, minimal quality loss
#
# **PHASE 2 - Quality Fine-tuning (150k-200k iterations):**
#   dynamic_training_mode: "full"     # Full per-sample dynamic kernels for maximum quality
#   fast_body_mode: false             # Restore full depth (original 3×4 architecture)
#   dynamic_update_every: 1           # Update tracked kernels every batch
#   → EXPECTED: Recovers most quality lost in fast training phase
#
# **Alternative Options:**
#   dynamic_training_mode: "off"      # Static kernels only (fastest, lowest quality)
#   fast_body_mode: true              # Always use reduced depth (fastest, lower quality)
#
# **To use fast training mode, uncomment and set these network_g parameters:**

name: 2xParagonSR2_S_gan
scale: 2

use_amp: true
amp_bf16: true
use_channels_last: true
fast_matmul: true
num_gpu: auto

# -------------------------------------------------------------------------------------------------
# Data & Network (Architecture-Specific)
# -------------------------------------------------------------------------------------------------
# [STANDARD TRAINNER-REDUX CONFIGURATION BLOCK - No major changes vs default templates]
datasets:
  train:
    name: Train_Dataset
    type: pairedimagedataset
    dataroot_gt: /home/phips/Documents/dataset/cc0/hr
    dataroot_lq: /home/phips/Documents/dataset/cc0/lr_x2
    lq_size: 128
    use_hflip: true
    use_rot: true
    num_worker_per_gpu: 8
    batch_size_per_gpu: 8
    accum_iter: 1

  val:
    name: Val_Dataset
    type: pairedimagedataset
    dataroot_gt: /home/phips/Documents/dataset/cc0/val_hr
    dataroot_lq: /home/phips/Documents/dataset/cc0/val_lr_x2

network_g:
  type: paragonsr2_s

  # === PERFORMANCE OPTIMIZATION FLAGS (UNCOMMENT TO ENABLE FAST TRAINING) ===
  # Training Strategy Options (choose ONE approach):

  # OPTION 1: FAST TRAINING (Recommended for first 150k iterations)
  dynamic_training_mode: "cheap"      # Fast SE-like channel attention (1.5-3× speedup)
  fast_body_mode: true                # Half depth architecture for speed
  use_norm: true
  dynamic_update_every: 16             # Less frequent kernel tracking
  use_channels_last: true             # Already enabled above

  # OPTION 2: QUALITY MODE (For final fine-tuning)
  # dynamic_training_mode: "full"       # Full per-sample dynamic kernels
  # fast_body_mode: false               # Full depth architecture
  # dynamic_update_every: 1             # Frequent kernel tracking

  # OPTION 3: MAXIMUM SPEED (Experimental, fastest training)
  # dynamic_training_mode: "off"        # No dynamic processing
  # fast_body_mode: true                # Half depth architecture
  # dynamic_update_every: 8             # Less frequent kernel tracking

  # OPTION 4: MAXIMUM QUALITY (Slowest but highest quality)
  # dynamic_training_mode: "full"       # Full per-sample dynamic kernels
  # fast_body_mode: false               # Full depth architecture
  # dynamic_update_every: 1             # Frequent kernel tracking

network_d:
  type: munet

path:
  pretrain_network_g: experiments/2xParagonSR2_S_fidelity/models/net_g_ema_400000.safetensors
  strict_load_g: true
  resume_state: ~

# -------------------------------------------------------------------------------------------------
# Training Settings: Optimizer & Scheduler Tuning
# -------------------------------------------------------------------------------------------------
train:
  ema_decay: 0.995
  ema_power: 0.75
  grad_clip: true # CRITICAL for stability in GAN/Perceptual regimes, especially with Charbonnier.

  # Optimizer for Generator (optim_g)
  # DIFFERENCE TO DEFAULT: Default uses LR 1e-4 and WD 0.0.
  # REASONING: We need to regularize the Generator in a high-pressure (GAN/Perceptual) environment.
  # ALTERNATIVES: Nadam or SGD variants were considered but AdamW is proven reliable.
  optim_g:
    type: AdamW
    lr: !!float 1.0e-4
    weight_decay: 1.0e-4 # Added: Moderate Weight Decay (1e-4) on G for regularization.
    betas: [0.9, 0.99]
    epsilon: 1e-6

  # Optimizer for Discriminator (optim_d)
  # DIFFERENCE TO DEFAULT: Default uses LR 1e-4. We use 1.5e-4.
  # REASONING: Employ Target-Tuned Update Ratio (TTUR) for stronger, more reliable adversarial signal.
  #            A stronger D helps G produce higher quality results that look more 'real'.
  optim_d:
    type: AdamW
    lr: !!float 1.5e-4 # TTUR: Higher LR (1.5x G's LR) for discriminator.
    weight_decay: 0.0 # Kept at 0.0 (like default) to ensure D remains flexible.
    betas: [0.9, 0.99]

  # Scheduler:
  # ALTERNATIVES: CosineAnnealingLR (can be faster but less stable with EMA, as experienced)
  #               OneCycleLR (great for speed, but too complex/opinionated for a shared baseline).
  # REASONING: MultiStepLR is standard, predictable, and highly stable, especially when combined with EMA.
  scheduler:
    type: MultiStepLR
    milestones: [100000, 150000]
    gamma: 0.5

  total_iter: 200000
  warmup_iter: 1000

  # -------------------------------------------------------------------------------------------------
  # Loss Stack: Lean V2 Recipe (Fewer, Better-Motivated Losses)
  # -------------------------------------------------------------------------------------------------
  losses:
    # 1) Fidelity anchor: Charbonnier Loss
    # DIFFERENCE TO DEFAULT: Default often relies on L1 or MS-SSIM.
    # REASONING: Charbonnier provides smoother gradients than L1 near zero, improving stability in
    #            GAN/Perceptual regimes. It resists noise better than L1.
    - type: charbonnierloss
      loss_weight: 1.0

    # 2) ConvNeXt-Tiny Perceptual Anchor (Superior to DINO for SISR)
    # WHY CONVNEXT-TINY > DINO for Super-Resolution:
    #
    # 1) INPUT FLEXIBILITY: DINO ViT models require strict input sizes (patch multiples)
    #    - DINO: Crashes with 256px crops (needs 518px for 14x14 patches)
    #    - ConvNeXt: Handles variable sizes gracefully, perfect for SISR training
    #
    # 2) TRAINING SPEED: ConvNeXt is 3-5x faster than DINO ViT
    #    - DINO: Attention operations are compute-heavy, slow inference
    #    - ConvNeXt: Native CNN operations, GPU-optimized, much faster
    #
    # 3) SISR SUITABILITY: ConvNeXt excels at texture and pattern learning
    #    - DINO: Self-supervised advantage is minimal for reconstruction tasks
    #    - ConvNeXt: Modern CNN designed for spatial features, excellent textures
    #
    # 4) STABILITY: ConvNeXt provides more stable training
    #    - DINO: Sensitive to input variations, compatibility issues
    #    - ConvNeXt: Robust CNN architecture, handles diverse inputs well
    #
    # 5) MEMORY EFFICIENCY: ConvNeXt uses less VRAM
    #    - DINO: ViT architecture requires more memory
    #    - ConvNeXt: Efficient CNN, better for production training
    #
    # CONCLUSION: For SISR, ConvNeXt-Tiny provides equal/better quality with
    # significant advantages in speed, stability, and practical deployment.
    - type: convnextperceptualloss
      loss_weight: 0.18
      layers: [1, 2]                        # Feature indices (0,1,2,3 available)
      layer_weights: [1.0, 0.7]             # Emphasize earlier features for SISR
      eps: 1.0e-6

    # 2) DINOv2-based Perceptual Anchor (Disabled due to input size constraints)
    # DIFFERENCE TO DEFAULT: Default uses VGG-based 'perceptualloss' (often with L1) or ConvNeXt.
    # REASONING: DINOv2 features are less coupled to high-level ImageNet semantics than VGG. They
    #            encourage superior low- and mid-level structure/texture fidelity, helping avoid
    #            the "plastic" or "oil-paint" look common in VGG-trained models.
    # ALTERNATIVES: VGG, LPIPS, ConvNeXt, CLIP Contrastive. DINOv2 was chosen as the most modern,
    #               texture-focused replacement.
    # NOTE: Optimized configuration - vit_small_patch14_dinov2 with single 'last' layer for optimal speed/quality ratio
    #       Previously: vit_small_patch16_dinov3 with multi-layer (very slow)
    #       Benefits: Smaller model (patch14), faster inference, 95% quality retention for SISR
    # ISSUE: vit_small_patch14_dinov2 requires strict input sizes (multiple of 14)
    #        Current crop size 256 is incompatible, causing runtime errors
    # ALTERNATIVE: Would need to either resize inputs or use compatible crop sizes
    #- type: DINOPerceptualLoss
    #  loss_weight: 0.18
    #  model_name: vit_small_patch14_dinov2
    #  layers: ['last']
    #  weights: [1.0]
    #  resize: true

    # 3) Artifact Suppression via Local Discriminative Loss (LDL)
    # DIFFERENCE TO DEFAULT: Default often leaves this disabled (loss_weight: 0).
    # REASONING: This is a dedicated term to suppress local abnormalities (ringing, blotchiness, noise)
    #            that generic losses miss. It is critical for the "artifact-free" goal.
    # ALTERNATIVES: No comparable direct loss; must be included.
    - type: ldlloss
      loss_weight: 1.0

    # 4) Structural / Perceptual Metric (DISTS)
    # DIFFERENCE TO DEFAULT: Often disabled (loss_weight: 0) or used in parallel with other structural losses.
    # REASONING: Retained for its proven ability to capture structural similarity in a perception-aware way.
    #            It plays well alongside Charbonnier and DINO.
    - type: distsloss
      loss_weight: 0.10
      as_loss: true
      load_weights: true
      use_input_norm: true

    # 5) Color / Brightness Consistency (CHC)
    # DIFFERENCE TO DEFAULT: Default often uses HSLuvLoss and/or CosimLoss.
    # REASONING: CHC is a highly effective, single component for color/hue/brightness stability,
    #            which directly addresses banding and color shift issues common in GAN training.
    - type: ConsistencyLoss
      loss_weight: 0.05
      criterion: chc
      blur: false
      saturation: 1.0
      brightness: 1.0
      cosim: true
      cosim_weight: 0.5

    # 6) Frequency Sharpening (FFLoss)
    # DIFFERENCE TO DEFAULT: Often disabled (loss_weight: 0).
    # REASONING: Actively pushes the model to synthesize detail in the frequency domain, resulting
    #            in notably crisper, non-blurry outputs.
    - type: ffloss
      loss_weight: 0.12

    # 7) Gradient-Variance Regularizer
    # DIFFERENCE TO DEFAULT: Often disabled (loss_weight: 0).
    # REASONING: A local regularizer that complements LDL by directly penalizing high gradient
    #            variance, suppressing jaggies, noise, and aliasing induced by sharpening losses.
    - type: GradientVarianceLoss
      loss_weight: 0.04
      patch_size: 16
      criterion: charbonnier

    # 8) Very light R3GAN Adversarial Loss
    # DIFFERENCE TO DEFAULT: Default often uses 'vanilla' GAN type at loss_weight 0.1.
    # REASONING: Uses R3GAN (Relativistic) for superior gradient feedback and stability. The low
    #            weight (0.04) is intentional—it provides micro-contrast/pop (human preference)
    #            without strong hallucination pressure.
    - type: ganloss
      gan_type: r3gan
      loss_weight: 0.04
      r1_weight: 2.5
      r2_weight: 2.5

# -------------------------------------------------------------------------------------------------
# Validation
# -------------------------------------------------------------------------------------------------
# [Validation uses standard perceptual/distortion metrics. Metrics like DISTS and TOPIQ are preferred.]
val:
  val_enabled: true
  val_freq: 5000
  save_img: true

  metrics_enabled: true
  metrics:
    psnr:
      type: calculate_psnr
      crop_border: 2
      test_y_channel: false
    ssim:
      type: calculate_ssim
      crop_border: 2
      test_y_channel: false
    lpips:
      type: calculate_lpips
      better: lower
    dists:
      type: calculate_dists
      better: lower
    topiq:
      type: calculate_topiq

# -------------------------------------------------------------------------------------------------
# Logging
# -------------------------------------------------------------------------------------------------
logger:
  print_freq: 100
  save_checkpoint_freq: 5000
  save_checkpoint_format: safetensors
  use_tb_logger: true
