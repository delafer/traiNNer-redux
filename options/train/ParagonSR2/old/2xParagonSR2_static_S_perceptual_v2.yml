# yaml-language-server: $schema=https://raw.githubusercontent.com/the-database/traiNNer-redux/refs/heads/master/schemas/redux-config.schema.json
# -------------------------------------------------------------------------------------------------
# ParagonSR2-STATIC-S (Perceptual Fine-Tuning v2) - High Frequency Detail Enhancement
# Uses fidelity model as pretrain with optimized ffloss for crisp microtexture learning
#
# TRAINING STRATEGY RATIONALE:
# This config addresses the artificial grid pattern issue by dramatically increasing
# frequency loss contribution for proper microtexture learning. Uses the fidelity
# model (net_g_ema_340000.safetensors) as pretrain and focuses on
# enhancing ffloss values to eliminate rasterization artifacts.
#
# KEY CHANGES FROM v1:
# - Much higher ffloss weights from start (0.25 → 0.60) to prevent grid artifacts
# - Earlier ffloss activation (0 iteration) for immediate microtexture enhancement
# - Reduced pixel loss dependency to let perceptual losses dominate
# - Optimized scheduling for crisp detail learning
#
# Author: Philip Hofmann
# Based on feedback from perceptual training with artificial grid issues
# -------------------------------------------------------------------------------------------------

name: 2xParagonSR2_S_perceptual_v2_microtexture
scale: 2

# -------------------------------------------------------------------------------------------------
# PERFORMANCE OPTIMIZATION FLAGS
# -------------------------------------------------------------------------------------------------
use_amp: true                          # Enable Automatic Mixed Precision for faster training
amp_bf16: true                        # Use BF16 for better numerical stability on Ampere+ GPUs
use_channels_last: true               # Optimize memory layout for better performance
fast_matmul: true                     # Trade precision for speed in matrix operations
num_gpu: auto                        # Automatically detect and use available GPUs

# -------------------------------------------------------------------------------------------------
# DATASET CONFIGURATION
# -------------------------------------------------------------------------------------------------
datasets:
  train:
    name: Train_Dataset
    type: pairedimagedataset
    dataroot_gt: /home/phips/Documents/dataset/cc0/hr
    dataroot_lq: /home/phips/Documents/dataset/cc0/lr_x2
    lq_size: 128                      # Input patch size - balanced for memory vs. context
    use_hflip: true                   # Horizontal flips for data augmentation
    use_rot: true                     # Rotations for better spatial invariance
    num_worker_per_gpu: 8             # Parallel data loading
    batch_size_per_gpu: 8             # Batch size per GPU - tuned for 8GB VRAM
    accum_iter: 1                     # Gradient accumulation steps

  val:
    name: Val_Dataset
    type: pairedimagedataset
    dataroot_gt: /home/phips/Documents/dataset/cc0/val_hr
    dataroot_lq: /home/phips/Documents/dataset/cc0/val_lr_x2

# -------------------------------------------------------------------------------------------------
# NETWORK ARCHITECTURE
# -------------------------------------------------------------------------------------------------
network_g:
  type: paragonsr2_static_s          # S-sized static ParagonSR2 for perceptual training

network_d:
  type: munet                        # MUNet discriminator for R3GAN

# -------------------------------------------------------------------------------------------------
# PRETRAINING & RESUME CONFIGURATION
# -------------------------------------------------------------------------------------------------
path:
  pretrain_network_g: experiments/2xParagonSR2_static_S_fidelity/models/net_g_ema_340000.safetensors # Use fidelity model as pretrain
  strict_load_g: true                # Load exact weights from fidelity model
  resume_state: ~                    # Fresh training from pretrain

# -------------------------------------------------------------------------------------------------
# TRAINING OPTIMIZATION SETTINGS
# -------------------------------------------------------------------------------------------------
train:
  ema_decay: 0.995                   # Exponential Moving Average decay for stable training
  ema_power: 0.75                   # EMA power for momentum calculation
  grad_clip: true                   # Enable gradient clipping for stability

  # Generator optimizer configuration
  optim_g:
    type: AdamW                      # AdamW for better weight decay handling
    lr: !!float 8.0e-5              # Slightly lower LR for fine-tuning from pretrain
    weight_decay: !!float 1.0e-4     # L2 regularization
    betas: [0.9, 0.99]              # AdamW beta parameters

  # Discriminator optimizer configuration
  optim_d:
    type: AdamW
    lr: !!float 1.2e-4              # Adjusted for perceptual training
    weight_decay: !!float 0.0       # No weight decay on discriminator
    betas: [0.9, 0.99]

  # Learning rate scheduler
  scheduler:
    type: MultiStepLR
    milestones: [60000, 100000]      # Adjusted milestones for 120k training
    gamma: 0.5                       # Reduce LR by 50% at each milestone

  total_iter: 120000                 # Total training iterations (120k) - focused training
  warmup_iter: 1000                  # Warmup iterations for stable start

  # -------------------------------------------------------------------------------------------------
  # OPTIMIZED LOSS SCHEDULING: 4-PHASE MICROTEXTURE ENHANCEMENT STRATEGY
  # -------------------------------------------------------------------------------------------------
  #
  # OVERALL PHILOSOPHY:
  # Build on pretrain model with immediate microtexture enhancement through high ffloss.
  # Focus on eliminating artificial grid patterns while maintaining perceptual quality.
  # Key difference: Much higher ffloss weights from start to prevent rasterization.
  #
  # EXPECTED VISUAL PROGRESSION:
  # Phase 1 (0-15k):   High ffloss for immediate microtexture learning, reduced pixel dependency
  # Phase 2 (15k-40k): Full perceptual enhancement with strong frequency domain details
  # Phase 3 (40k-80k): Fine-tuning balance, artifact removal
  # Phase 4 (80k+):    Final quality polish with maintained microtexture quality
  #
  # CRITICAL FFLOSS OPTIMIZATION:
  # - Start with 0.25 weight from iteration 0 (vs previous 0.15 from 114k)
  # - Target 0.60 weight by 40k (vs previous 0.35 by 120k)
  # - This should yield ffloss values ~1e-4 to 1e-3 range for proper microtexture learning
  # -------------------------------------------------------------------------------------------------
  losses:

    # ===============================================================================================
    # 1) REDUCED PIXEL LOSS - MINIMAL STRUCTURAL ANCHOR (PHASE 1 → 2 REDUCTION)
    # ===============================================================================================
    #
    # PURPOSE: Minimal structural anchor while letting perceptual losses dominate
    # RATIONALE: With pretrain model, less pixel accuracy needed. Focus on perceptual quality.
    #
    # SCHEDULE DECISION:
    # - loss_weight: 0.6 → Reduced from 1.0 to let perceptual losses dominate
    # - start_iter: 0 → Active immediately but with reduced influence
    # - target_iter: 15000 → Quick reduction to minimize pixel dependency
    # - target_weight: 0.4 → Further reduced to 0.4 for strong perceptual focus
    # - schedule_type: linear → Smooth transition to avoid training instability
    #
    # WHY THIS MATTERS: Reduced pixel loss allows frequency and perceptual losses to better
    #                  enhance microtexture details without being constrained by pixel accuracy.
    #                  This is critical for eliminating the artificial grid pattern.
    # ===============================================================================================
    - type: charbonnierloss
      loss_weight: 0.6               # Reduced from 1.0 for perceptual dominance
      start_iter: 0                  # Active but reduced influence
      target_iter: 15000            # Quick reduction to minimal dependency
      target_weight: 0.4             # Further reduced for perceptual focus
      schedule_type: linear          # Smooth transition to avoid instability

    # ===============================================================================================
    # 2) CONVNEXT PERCEPTUAL LOSS - ENHANCED PERCEPTUAL BASE (PHASE 1 ACTIVATION)
    # ===============================================================================================
    #
    # PURPOSE: Enhanced perceptual learning for better texture-structure balance
    # RATIONALE: Slightly increased weights for better overall perceptual quality
    #
    # SCHEDULE DECISION:
    # - loss_weight: 0.15 → Increased from 0.12 for stronger perceptual foundation
    # - start_iter: 0 → Active from beginning for immediate perceptual enhancement
    # - target_iter: 20000 → Ramp up to strong perceptual influence
    # - target_weight: 0.30 → Increased target for enhanced perceptual quality
    # - layers: [1, 2] → Maintain optimal layer selection
    # - layer_weights: [1.0, 0.7] → Balanced layer weighting
    #
    # WHY THIS MATTERS: Enhanced perceptual foundation complements the high ffloss for
    #                  overall superior texture quality and structural coherence.
    # ===============================================================================================
    - type: convnextperceptualloss
      loss_weight: 0.15              # Increased for stronger perceptual foundation
      layers: [1, 2]                 # Optimal perceptual layers
      layer_weights: [1.0, 0.7]      # Balanced layer weighting
      eps: 1.0e-6                   # Numerical stability
      start_iter: 0                  # Active from beginning
      target_iter: 20000            # Strong perceptual enhancement ramp
      target_weight: 0.30           # Enhanced target for better quality

    # ===============================================================================================
    # 3) DISTS LOSS - STRUCTURAL PERCEPTUAL COMPLEMENT (FULL DURATION)
    # ===============================================================================================
    #
    # PURPOSE: Consistent perceptual enhancement throughout training
    # RATIONALE: Maintain stable perceptual foundation with slight increase
    #
    # SCHEDULE DECISION:
    # - loss_weight: 0.10 → Slight increase from 0.08 for better perceptual balance
    # - Active throughout training for consistent perceptual enhancement
    # - as_loss: true → Direct loss function usage
    # - load_weights: true → Load pretrained weights
    # - use_input_norm: true → Normalize inputs
    #
    # WHY THIS MATTERS: Consistent perceptual enhancement complements high ffloss
    #                  for comprehensive texture and structure quality.
    # ===============================================================================================
    - type: distsloss
      loss_weight: 0.10              # Slight increase for better perceptual balance
      as_loss: true                  # Direct loss function
      load_weights: true            # Load pretrained weights
      use_input_norm: true          # Normalize inputs

    # ===============================================================================================
    # 4) ENHANCED FREQUENCY LOSS - CRITICAL MICROTEXTURE LEARNING (PHASE 1 → 2)
    # ===============================================================================================
    #
    # PURPOSE: Eliminate artificial grid pattern through proper microtexture learning
    # RATIONALE: MUCH higher ffloss weights from start to ensure proper microdetail learning.
    #           This is the key fix for the rasterization/artificial grid issue.
    #
    # SCHEDULE DECISION:
    # - loss_weight: 0.25 → DRAMATICALLY increased from 0.15 for immediate effect
    # - start_iter: 0 → Activate immediately for instant microtexture enhancement
    # - target_iter: 40000 → Strong ramp over perceptual training phase
    # - target_weight: 0.60 → MASSIVE increase to 0.60 vs previous 0.35
    # - schedule_type: linear → Smooth but aggressive enhancement
    #
    # EXPECTED FFLOSS VALUES:
    # - With 0.25 initial weight, expect ffloss around 5e-4 to 1e-3
    # - With 0.60 target weight, expect ffloss around 1e-3 to 5e-3
    # - These values should be sufficient to learn fine texture patterns
    #
    # WHY THIS MATTERS: This is THE critical fix. The previous config had ffloss values
    #                  around 3e-5 which is far too low for microtexture learning.
    #                  These new weights should yield ffloss values 10-100x higher,
    #                  enabling proper learning of fine texture patterns and elimination
    #                  of artificial grid patterns on detailed surfaces.
    # ===============================================================================================
    - type: ffloss
      loss_weight: 0.25              # DRAMATICALLY increased from 0.15 for immediate effect
      start_iter: 0                  # IMMEDIATE activation - critical for microtexture
      target_iter: 40000            # Strong ramp during perceptual phase
      target_weight: 0.60           # MASSIVE increase to 0.60 vs previous 0.35
      schedule_type: linear         # Smooth but aggressive enhancement

    # ===============================================================================================
    # 5) GRADIENT VARIANCE LOSS - MAINTAINED TEXTURE REGULARITY (EARLY ACTIVATION)
    # ===============================================================================================
    #
    # PURPOSE: Prevent over-sharpening while allowing enhanced detail
    # RATIONALE: Slightly increased weight to complement high ffloss
    #
    # SCHEDULE DECISION:
    # - loss_weight: 0.04 → Slight increase from 0.03 for better texture control
    # - start_iter: 500 → Earlier activation for immediate texture stability
    # - patch_size: 16 → Maintain optimal patch size for detail analysis
    # - criterion: charbonnier → Robust texture analysis
    #
    # WHY THIS MATTERS: Higher gradient variance loss helps prevent artifacts that might
    #                  occur with the dramatically increased ffloss. Early activation
    #                  ensures texture regularity is maintained throughout enhancement.
    # ===============================================================================================
    - type: GradientVarianceLoss
      loss_weight: 0.04              # Slight increase for better texture control
      start_iter: 500                # Earlier activation for stability
      patch_size: 16                # Optimal patch size for detail analysis
      criterion: charbonnier        # Robust texture analysis

    # ===============================================================================================
    # 6) LDL LOSS - ARTIFACT SUPPRESSION (PHASE 2 → 3 TRANSITION)
    # ===============================================================================================
    #
    # PURPOSE: Suppress artifacts during intense perceptual training
    # RATIONALE: Slightly reduced weight to allow more aggressive perceptual enhancement
    #
    # SCHEDULE DECISION:
    # - loss_weight: 0.4 → Reduced from 0.5 to allow stronger perceptual enhancement
    # - disable_after: 60000 → Extended duration for longer perceptual training
    #
    # WHY THIS MATTERS: Reduced LDL allows the high ffloss to work more aggressively
    #                  while still providing artifact suppression during training.
    #                  Extended duration covers the full perceptual enhancement phase.
    # ===============================================================================================
    - type: ldlloss
      loss_weight: 0.4               # Reduced to allow stronger perceptual enhancement
      disable_after: 60000          # Extended duration for perceptual training

    # ===============================================================================================
    # 7) R3GAN LOSS - MODERATE ADVERSARIAL ENHANCEMENT (PHASE 2 ACTIVATION)
    # ===============================================================================================
    #
    # PURPOSE: Generate realistic textures with minimal interference to ffloss
    # RATIONALE: Slightly reduced weight to let ffloss dominate microtexture learning
    #
    # SCHEDULE DECISION:
    # - loss_weight: 0.00 → Base weight
    # - start_iter: 15000 → Slightly delayed for ffloss establishment
    # - target_weight: 0.06 → Reduced from 0.08 for less interference
    # - gan_type: r3gan → Maintain R3GAN for stability
    # - r1_weight: 2.0 → Strong regularization
    # - r2_weight: 2.0 → Balanced adversarial training
    #
    # WHY THIS MATTERS: Reduced adversarial weight prevents GAN from interfering
    #                  with the critical ffloss microtexture learning phase.
    # ===============================================================================================
    - type: ganloss
      gan_type: r3gan
      loss_weight: 0.00
      start_iter: 15000             # Slightly delayed for ffloss establishment
      target_weight: 0.06           # Reduced for less interference with ffloss
      r1_weight: 2.0               # Strong regularization
      r2_weight: 2.0               # Balanced training

    # ===============================================================================================
    # 8) CONTRASTIVE LOSS - SEMANTIC POLISH (LATE ACTIVATION)
    # ===============================================================================================
    #
    # PURPOSE: Final semantic enhancement without interfering with microtexture learning
    # RATIONALE: Maintained late activation and reduced weight
    #
    # SCHEDULE DECISION:
    # - loss_weight: 0.00 → Base weight
    # - start_iter: 35000 → Later activation to avoid ffloss interference
    # - target_weight: 0.03 → Reduced from 0.05 for subtle enhancement
    # - temperature: 0.1 → Sharp contrastive features
    #
    # WHY THIS MATTERS: Late, reduced activation ensures contrastive learning doesn't
    #                  interfere with the critical ffloss microtexture enhancement phase.
    # ===============================================================================================
    - type: ContrastiveLoss
      loss_weight: 0.00
      temperature: 0.1              # Sharp contrastive features
      start_iter: 35000             # Later activation to avoid interference
      target_weight: 0.03           # Reduced for subtle enhancement

# -------------------------------------------------------------------------------------------------
# VALIDATION CONFIGURATION
# -------------------------------------------------------------------------------------------------
val:
  val_enabled: true                 # Enable validation during training
  val_freq: 5000                   # Validate every 5k iterations
  save_img: true                   # Save validation images for progress monitoring
  metrics_enabled: true            # Enable automatic metric calculation

  # Comprehensive metric suite for quality assessment
  metrics:
    psnr:                          # Peak Signal-to-Noise Ratio (fidelity metric)
      type: calculate_psnr
      crop_border: 2               # Standard crop for fair comparison

    ssim:                          # Structural Similarity Index (structural quality)
      type: calculate_ssim
      crop_border: 2

    lpips:                         # Learned Perceptual Image Patch Similarity (perceptual quality)
      type: calculate_lpips
      better: lower               # Lower LPIPS = better perceptual quality

    dists:                         # Deep Image Structure and Texture Similarity
      type: calculate_dists
      better: lower

    topiq:                         # Task-Oriented Perceptual Image Quality
      type: calculate_topiq

# -------------------------------------------------------------------------------------------------
# LOGGING CONFIGURATION
# -------------------------------------------------------------------------------------------------
logger:
  print_freq: 100                  # Print training progress every 100 iterations
  save_checkpoint_freq: 5000       # Save model checkpoints every 5k iterations
  save_checkpoint_format: safetensors # Use SafeTensors format for better compatibility
  use_tb_logger: true              # Enable TensorBoard logging for detailed monitoring

# -------------------------------------------------------------------------------------------------
# CRITICAL TRAINING EXPECTATIONS & MONITORING POINTS
# -------------------------------------------------------------------------------------------------
#
# EXPECTED VISUAL PROGRESSION WITH HIGH FFLOSS:
# - 5k iterations: Immediate microtexture enhancement visible
# - 15k iterations: Significant reduction in artificial grid patterns
# - 40k iterations: Full microtexture learning with crisp details
# - 80k iterations: Mature model with superior texture quality
#
# KEY MONITORING METRICS:
# - ffloss values: Should be 5e-4 to 5e-3 range (vs previous 3e-5)
# - LPIPS: Should show significant improvement (perceptual enhancement)
# - Visual inspection: Artificial grid should disappear, textures should be crisp
# - PSNR: May decrease slightly but acceptable for perceptual gain
#
# CRITICAL SUCCESS INDICATORS:
# - ffloss contribution: Should be significantly higher than previous run
# - Texture detail: Fine patterns should be crisp without pixelation
# - Grid artifacts: Should be eliminated on detailed surfaces
# - Overall quality: Visually superior to previous perceptual model
#
# QUALITY CHECKPOINTS:
# - 10k: Check for ffloss values in expected range (5e-4+)
# - 20k: Verify microtexture enhancement without artifacts
# - 40k: Confirm crisp detail learning, reduced grid artifacts
# - 80k: Mature quality assessment vs previous model
#
# TROUBLESHOOTING:
# - If ffloss still too low: Consider increasing weights further
# - If artifacts appear: Slightly reduce ffloss ramp speed
# - If training unstable: Reduce learning rate or increase warmup
#
# This configuration specifically addresses the artificial grid issue through
# aggressive frequency loss enhancement while maintaining overall training stability.
# The key improvement is 10-100x higher expected ffloss values for proper microtexture learning.
# -------------------------------------------------------------------------------------------------
