# yaml-language-server: $schema=https://raw.githubusercontent.com/the-database/traiNNer-redux/refs/heads/master/schemas/redux-config.schema.json
#########################################################################################
# Fidelity Pretrain Config for ParagonSR2 Hybrid S
# Goals: Clean fidelity pretrain, artifact-free, fast, deployable
#########################################################################################
name: 2xParagonSR2_S_Hybrid_Fidelity
scale: 2

use_amp: true # Enable automatic mixed precision
amp_bf16: true # Use bf16 instead of fp16 for supported GPUs
use_channels_last: true # Memory-friendly format for speed
fast_matmul: false # Precision-over-speed tradeoff
num_gpu: auto # Automatically detect available GPUs
manual_seed: 1024 # Optional: fixes randomness for reproducibility

#########################################################################################
# Dataset & Dataloader Settings
#########################################################################################
datasets:
  train:
    name: Train Dataset
    type: pairedimagedataset
    dataroot_gt: [/home/phips/Documents/dataset/cc0/hr]
    dataroot_lq: [/home/phips/Documents/dataset/cc0/lr_x2] # Clean LR (no JPEG)
    lq_size: 128
    use_hflip: true
    use_rot: true
    num_worker_per_gpu: 8
    batch_size_per_gpu: 6 # Hybrid is faster, can increase batch size
    accum_iter: 1

  val:
    name: Val Dataset
    type: pairedimagedataset
    dataroot_gt: [/home/phips/Documents/dataset/cc0/val_hr]
    dataroot_lq: [/home/phips/Documents/dataset/cc0/val_lr_x2]

#########################################################################################
# Generator Network
#########################################################################################
network_g:
  type: paragonsr2_s
  scale: 2
  # Hybrid architecture uses sensible defaults:
  # - 48 features, 3 groups, 4 blocks
  # - upsampler_alpha: 0.5 (MagicKernel sharpening)
  # - detail_gain: 0.1 (initial detail contribution)
  # - fast_body_mode: true (faster training, slight quality tradeoff)

#########################################################################################
# Pretrain & Resume Paths
#########################################################################################
path:
  param_key_g: ~
  strict_load_g: true # Ensures weights are loaded only if names match exactly
  resume_state: ~

#########################################################################################
# Training Settings
#########################################################################################
train:
  ema_decay: 0.999
  ema_power: 0.75
  grad_clip: true

  # Optimizer for Generator
  optim_g:
    type: AdamW
    lr: !!float 1e-4 # Slightly lower for stability
    weight_decay: !!float 1e-4 # Mild regularization
    betas: [0.9, 0.99]

  # Scheduler: MultiStepLR for stable decay
  scheduler:
    type: MultiStepLR
    milestones: [200000, 300000, 350000]
    gamma: 0.5

  total_iter: 400000
  warmup_iter: 1000 # Gradually ramp up LR for stable early training

  # Loss stack: minimal fidelity-focused
  losses:
    # Charbonnier Loss: smooth gradient, robust to noise
    - type: charbonnierloss
      loss_weight: 1.0

#########################################################################################
# Validation Settings
#########################################################################################
val:
  val_enabled: true
  val_freq: 5000
  save_img: true
  tile_size: 0
  tile_overlap: 8

  metrics_enabled: true
  metrics:
    psnr:
      type: calculate_psnr
      crop_border: 4
      test_y_channel: false
    ssim:
      type: calculate_ssim
      crop_border: 4
      test_y_channel: false

#########################################################################################
# Logging
#########################################################################################
logger:
  print_freq: 100
  save_checkpoint_freq: 5000
  save_checkpoint_format: safetensors
  use_tb_logger: true
