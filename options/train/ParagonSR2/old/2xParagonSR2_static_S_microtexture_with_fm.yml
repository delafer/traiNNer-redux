# =============================================================================
# ParagonSR2 Static-S Microtexture Training Configuration
# =============================================================================
# This configuration is specifically designed for high-quality 2x super-resolution
# with emphasis on perceptual quality and microtexture detail preservation.
# It combines multiple loss functions to achieve excellent perceptual scores
# while maintaining good reconstruction fidelity.

# Training run identification
name: 2xParagonSR2_S_with_FeatureMatching

# Super-resolution scale factor (2x = 2x the resolution)
scale: 2

# =============================================================================
# TRAINING ACCELERATION SETTINGS
# =============================================================================
# These settings optimize training speed and memory usage without quality loss

# Automatic Mixed Precision - Speeds up training and reduces memory usage
use_amp: true

# Brain Float 16 - More numerically stable than regular FP16, prevents NaN/Inf
# Particularly important for deep networks and gradient accumulation
amp_bf16: true

# Channels Last Memory Format - Optimizes GPU memory access patterns
# Especially beneficial for convolution-heavy architectures like ParagonSR2
use_channels_last: true

# Fast Matrix Operations - Trades small precision for significant speed gains
# Uses TensorFloat32 for matrix multiplications on modern NVIDIA GPUs
fast_matmul: true

# Auto-detect GPU count - Uses all available GPUs for training
num_gpu: auto

# =============================================================================
# DATASET CONFIGURATION
# =============================================================================
# Paired image dataset for supervised learning

datasets:
  # Training dataset configuration
  train:
    name: Train_Dataset
    type: pairedimagedataset

    # Data paths - replace with your actual dataset paths
    dataroot_gt: /path/to/your/hr/train    # Ground truth (high-resolution) images directory
    dataroot_lq: /path/to/your/lr/train    # Low-quality (input) images directory

    # Input image size - 128x128 patches for efficient training
    # Balance between detail capture and computational efficiency
    lq_size: 128

    # Data augmentation - Horizontal flipping
    # Doubles effective dataset size, improves generalization
    use_hflip: true

    # Data augmentation - Random rotation (90°, 180°, 270°)
    # Helps with orientation invariance, especially for texture patterns
    use_rot: true

    # Data loading workers per GPU - Loads data in parallel
    # Higher values improve data pipeline efficiency
    num_worker_per_gpu: 8

    # Batch size per GPU - Total batch size = batch_size_per_gpu * num_gpu
    # 8 is optimal for 2x SR on modern GPUs with AMP enabled
    batch_size_per_gpu: 8

    # Gradient accumulation steps - Simulates larger batch size
    # accum_iter: 1 means no accumulation (each batch updates gradients)
    accum_iter: 1

  # Validation dataset configuration
  val:
    name: Val_Dataset
    type: pairedimagedataset
    dataroot_gt: /path/to/your/hr/val
    dataroot_lq: /path/to/your/lr/val
    # Note: No data augmentation for validation - we want consistent metrics

# =============================================================================
# NETWORK ARCHITECTURES
# =============================================================================
# Generator and discriminator networks for adversarial training

# Generator: ParagonSR2 Static-S architecture
# Optimized for speed while maintaining high-quality results
# Static-S variant uses simplified attention mechanisms for faster inference
network_g:
  type: paragonsr2_static_s

# Discriminator: MUNet (Multi-Branch UNet)
# Multi-branch design captures different aspects of image quality:
# - Spatial branch: Overall structure and layout
# - Frequency branch: Texture and frequency content
# - Patch branch: Local texture patterns
network_d:
  type: munet

# =============================================================================
# MODEL LOADING AND SAVING
# =============================================================================
path:
  # Pretrained generator weights - None for training from scratch
  # Set to checkpoint path for fine-tuning
  pretrain_network_g: ~  # ~ means null/None in YAML

  # Strict loading ensures parameter names must match exactly
  # Prevents loading incompatible checkpoint files
  strict_load_g: true

  # Resume training from checkpoint - None for fresh training
  resume_state: ~

# =============================================================================
# TRAINING SETTINGS
# =============================================================================
train:
  # Exponential Moving Average for generator weights
  # Creates a smoother, more stable version of the generator
  # Helps prevent training instability and improves generalization
  # EMA decay rate - 0.995 means 99.5% of previous weights retained
  ema_decay: 0.995

  # EMA power - Controls how quickly EMA model updates
  # 0.75 is a good balance for stable training
  ema_power: 0.75

  # Gradient clipping - Prevents exploding gradients
  # Clips gradients to maximum norm of 1.0
  grad_clip: true

  # =============================================================================
  # OPTIMIZER CONFIGURATION
  # =============================================================================
  # Separate optimizers for generator and discriminator

  # Generator optimizer - AdamW (Adam with weight decay)
  # Better than regular Adam for image generation tasks
  optim_g:
    type: AdamW

    # Learning rate - 1.2e-4 is conservative for stable training
    # Too high leads to instability, too low slows convergence
    lr: !!float 1.2e-4

    # Weight decay - L2 regularization to prevent overfitting
    # 1.0e-4 is moderate, helps generalization
    weight_decay: !!float 1.0e-4

    # Adam hyperparameters - [beta1, beta2]
    # beta1=0.9: momentum term for gradient accumulation
    # beta2=0.99: second moment estimation for adaptive learning rates
    betas: [0.9, 0.99]

  # Discriminator optimizer - Higher learning rate for faster adaptation
  # Discriminators often need to adapt faster than generators
  optim_d:
    type: AdamW

    # Slightly higher LR than generator (1.5e-4 vs 1.2e-4)
    # Discriminator needs to keep up with generator improvements
    lr: !!float 1.5e-4

    # No weight decay for discriminator
    # Can hurt discriminator's ability to distinguish real vs fake
    weight_decay: !!float 0.0

    betas: [0.9, 0.99]

  # =============================================================================
  # LEARNING RATE SCHEDULING
  # =============================================================================
  # MultiStepLR - Drops learning rate at specific milestones
  # Helps fine-tune the model at different training stages
  scheduler:
    type: MultiStepLR

    # Learning rate drops at these iterations
    # 70k: Focus shifts from reconstruction to perceptual quality
    # 110k: Final fine-tuning for best perceptual scores
    milestones: [70000, 110000]

    # Gamma - Multiplicative factor for LR reduction
    # 0.5 means LR is halved at each milestone
    gamma: 0.5

  # Total training iterations - 140k provides good training depth
  # Enough iterations to converge without overfitting
  total_iter: 140000

  # Warmup iterations - Gradual learning rate increase
  # Prevents instability at the beginning of training
  warmup_iter: 500

  # =============================================================================
  # LOSS FUNCTION CONFIGURATION
  # =============================================================================
  # Multi-component loss for optimal perceptual quality

  losses:

    # =============================================================================
    # 1. RECONSTRUCTION LOSSES - Ensure faithful image reconstruction
    # =============================================================================

    # Charbonnier Loss - Robust reconstruction loss with smooth gradients
    # Primary loss for pixel-wise accuracy and basic structure
    # Starts strong (0.6) to establish good reconstruction, gradually reduces to 0.4
    # Smooth gradients prevent training instability while maintaining fidelity
    - type: charbonnierloss
      loss_weight: 0.6
      start_iter: 0
      target_iter: 25000    # Gradually reduce over first 25k iterations
      target_weight: 0.4    # Final weight after 25k iterations
      schedule_type: linear # Linear interpolation between weights

    # ConvNeXt Perceptual Loss - High-level semantic understanding
    # Uses pre-trained ConvNeXt features for perceptual similarity
    # Layers [1,2] capture mid-level features (edges, textures, patterns)
    # Layer weights [1.0, 0.7] give more importance to earlier layers
    # Essential for natural-looking textures and realistic details
    - type: convnextperceptualloss
      loss_weight: 0.16
      layers: [1, 2]        # Use layers 1 and 2 from ConvNeXt
      layer_weights: [1.0, 0.7]  # Weight for each layer (earlier = more important)
      eps: 1.0e-6           # Small value for numerical stability
      start_iter: 0
      target_iter: 25000
      target_weight: 0.32   # Reduces to maintain balance with other losses

    # DISTS Loss - Deep Image Structure and Texture Similarity
    # Uses learned features to measure perceptual similarity
    # Particularly good at capturing texture and structural patterns
    # More robust than simple L1/L2 losses for perceptual quality
    # Load pre-trained weights for consistent feature representation
    - type: distsloss
      loss_weight: 0.10
      as_loss: true         # Use as loss function (not metric)
      load_weights: true    # Load pre-trained DISTS weights
      use_input_norm: true  # Normalize inputs for better feature extraction

    # =============================================================================
    # 2. FREQUENCY DOMAIN LOSSES - Enhance texture and detail preservation
    # =============================================================================

    # Frequency Feature Loss (FF Loss) - Frequency domain guidance
    # Encourages generator to preserve frequency content correctly
    # Very important for texture detail and microstructures
    # Grows from 0.40 to 0.55 over 90k iterations as emphasis shifts to texture
    - type: ffloss
      loss_weight: 0.40
      start_iter: 0
      target_iter: 90000    # Gradually increase frequency emphasis
      target_weight: 0.55
      schedule_type: linear

    # Gradient Variance Loss - Preserves fine texture details
    # Encourages smooth gradients that capture micro-textures
    # Especially important for material textures and surface details
    # Wait 500 iterations before starting to avoid early instability
    - type: GradientVarianceLoss
      loss_weight: 0.05
      start_iter: 500       # Wait 500 iterations before starting
      patch_size: 16        # Local patch size for gradient computation
      criterion: charbonnier # Robust loss function for smooth gradients

    # =============================================================================
    # 3. FEATURE MATCHING LOSS - NEW! Multi-branch discriminator guidance
    # =============================================================================
    # This loss encourages the generator to match discriminator's internal features
    # Particularly effective with MUNet's multiple branches (spatial, frequency, patch)
    # Provides strong training stabilization and reduces hallucinations/artifacts

    - type: featurematchingloss
      loss_weight: 0.10     # Strong stabilizer, helps reduce spotty hallucinations
      layers: [ 'down1', 'down2', 'mid' ]  # Key MUNet discriminator layers:
                           # down1: Early encoder features (basic patterns)
                           # down2: Mid-level encoder features (local textures)
                           # mid: Bottleneck features (global structure)
      criterion: charbonnier # Robust criterion for stable feature matching
      start_iter: 0         # Active from the beginning for stabilization

    # =============================================================================
    # 4. LOCAL DISCRIMINATOR LEARNING (LDL) - Self-supervision
    # =============================================================================
    # LDL encourages the generator to produce images that are harder to distinguish
    # from real images by a local discriminator
    # Disabled after 90k iterations to focus on global quality
    - type: ldlloss
      loss_weight: 0.3
      disable_after: 90000

    # =============================================================================
    # 5. HIGH-FREQUENCY PRESERVATION - Detail enhancement
    # =============================================================================

    # HFEN Loss - High-Frequency Error Norm
    # Preserves sharp edges and fine details using Laplacian filtering
    # Kernel size 7 provides good frequency response for edge detection
    # Sigma 1.0 controls edge sharpness in the filter
    # Small weight but important for maintaining crisp details
    - type: hfenloss
      loss_weight: 0.015    # Small weight, but important for sharpness
      kernel_size: 7        # Filter kernel size for edge detection
      sigma: 1.0           # Gaussian sigma for smoothing
      criterion: charbonnier # Robust loss for edge preservation
      reduction: mean       # Average across spatial dimensions
      start_iter: 0

    # =============================================================================
    # 6. ARTIFACT REDUCTION - Smoothness and quality control
    # =============================================================================

    # Adaptive Block Total Variation Loss - Reduces artifacts while preserving edges
    # Block size 2 provides local smoothness to remove noise
    # Sharpness 4.0 controls the edge-preserving strength
    # Very important for removing GAN artifacts while maintaining texture
    - type: AdaptiveBlockTVLoss
      loss_weight: 0.004    # Small weight to avoid over-smoothing
      block_size: 2         # Local block size for TV computation
      sharpness: 4.0       # Higher values preserve edges better
      reduction: mean       # Average loss across spatial dimensions
      eps: 1e-6            # Numerical stability for small gradients
      start_iter: 0

    # =============================================================================
    # 7. ADVERSARIAL TRAINING - Introduced later for photorealism
    # =============================================================================

    # GAN Loss - R3GAN (Regularized R1 GAN) with gradient penalties
    # Starts later to avoid early training instability
    # R1/R2 regularization helps prevent mode collapse and improves stability
    # Gradual introduction allows model to learn basic reconstruction first
    - type: ganloss
      gan_type: r3gan       # R3GAN with built-in regularization
      loss_weight: 0.00     # Starts at 0, gradually increases
      start_iter: 30000     # Wait 30k iterations before starting GAN
      target_weight: 0.06   # Final adversarial strength for photorealism
      r1_weight: 2.0        # R1 regularization strength (penalizes real image gradients)
      r2_weight: 2.0        # R2 regularization strength (penalizes fake image gradients)
      schedule_type: linear # Gradual weight increase for stability
      target_iter: 45000    # Reach target weight by 45k iterations

    # =============================================================================
    # 8. CONTRASTIVE LEARNING - Late-stage enhancement
    # =============================================================================

    # Contrastive Loss - Uses CLIP features to encourage semantic similarity
    # Started later (60k iterations) when basic reconstruction is solid
    # Helps with semantic consistency and improves perceptual quality
    # Temperature 0.1 controls the sharpness of the contrastive learning
    - type: ContrastiveLoss
      loss_weight: 0.00     # Starts at 0, gradually increases
      temperature: 0.1      # Controls contrastive learning sharpness
      start_iter: 60000     # Wait 60k iterations before starting
      target_weight: 0.05   # Final contrastive strength

# =============================================================================
# VALIDATION AND EVALUATION
# =============================================================================
val:
  # Enable validation during training
  val_enabled: true

  # Validation frequency - Run validation every 5000 iterations
  # Balance between monitoring training progress and training speed
  val_freq: 5000

  # Save validation images for visual inspection
  save_img: true

  # Enable comprehensive metrics calculation
  metrics_enabled: true

  # Metrics for evaluating both reconstruction quality and perceptual quality
  metrics:
    # PSNR - Peak Signal-to-Noise Ratio (pixel-wise reconstruction fidelity)
    psnr:
      type: calculate_psnr
      crop_border: 2       # Ignore 2-pixel border (avoids boundary artifacts)

    # SSIM - Structural Similarity Index (structural preservation)
    ssim:
      type: calculate_ssim
      crop_border: 2       # Consistent crop for fair comparison

    # LPIPS - Learned Perceptual Image Patch Similarity (perceptual quality)
    lpips:
      type: calculate_lpips
      better: lower        # Lower LPIPS = better perceptual quality

    # DISTS - Deep Image Structure and Texture Similarity (texture quality)
    dists:
      type: calculate_dists
      better: lower        # Lower DISTS = better structure/texture match

    # ToPIQ - Transferable Perceptual Image Quality (comprehensive quality)
    topiq:
      type: calculate_topiq

# =============================================================================
# LOGGING AND MONITORING
# =============================================================================
logger:
  # Console logging frequency - Print training status every 100 iterations
  print_freq: 100

  # Save model checkpoints every 5000 iterations
  # Critical for resuming training and model evaluation
  save_checkpoint_freq: 5000

  # Use modern checkpoint format - SafeTensors is more secure and portable
  save_checkpoint_format: safetensors

  # TensorBoard logging for loss curves and image visualization
  # Essential for monitoring training progress and debugging
  use_tb_logger: true
