#########################################################################################
# ParagonSR2 Nano - MINIMAL Zero-Config Template
#
# This is the ABSOLUTE MINIMUM config needed. Everything else is auto-detected!
#
# USAGE:
# 1. Fill in your dataset paths below (marked with ğŸ“)
# 2. Specify architecture (marked with ğŸ¯)
# 3. Run training - framework auto-detects everything else!
#
# The framework will automatically:
# âœ… Detect your hardware (GPU, VRAM, CPU)
# âœ… Calculate optimal batch size
# âœ… Set all automation parameters
# âœ… Configure AMP/precision settings
# âœ… Optimize learning rates and schedules
# âœ… Set safety bounds and fallbacks
#########################################################################################

# Basic required settings (only 3 things needed!)
name: ParagonSR2_Nano_ZeroConfig_Minimal
scale: 2                                          # ğŸ¯ Architecture scale factor

# Dataset configuration (ONLY paths need to be specified)
datasets:
  train:
    name: ZeroConfig_Train
    type: pairedimagedataset
    dataroot_gt: /home/phips/Documents/dataset/cc0/hr          # ğŸ“ USER: Your HR training path
    dataroot_lq: /home/phips/Documents/dataset/cc0/lr_x2_bicubic_aa  # ğŸ“ USER: Your LR training path
    # âœ… All other settings auto-optimized

  val:
    name: ZeroConfig_Val
    type: pairedimagedataset
    dataroot_gt: /home/phips/Documents/dataset/cc0/val_hr     # ğŸ“ USER: Your HR validation path
    dataroot_lq: /home/phips/Documents/dataset/cc0/val_lr_x2_bicubic_aa  # ğŸ“ USER: Your LR validation path
    # âœ… All other settings auto-optimized

# Network configuration (ONLY architecture type needed)
network_g:
  type: paragonsr2_nano                          # ğŸ¯ USER: Architecture name

# That's it! Everything below is AUTOMATIC...
# The framework auto-detects and optimizes all training parameters

# AUTO-DETECTED SYSTEM SETTINGS (optimized for your hardware):
# âœ… use_amp: true/false (auto-detected from GPU capabilities)
# âœ… amp_bf16: true/false (auto-selected based on GPU support)
# âœ… use_channels_last: true (auto-optimized for GPU)
# âœ… fast_matmul: true (auto-enabled if supported)
# âœ… num_gpu: auto (auto-detect available GPUs)

# AUTO-DETECTED DATASET SETTINGS:
# âœ… lq_size: auto (optimized for architecture)
# âœ… batch_size_per_gpu: auto (calculated from VRAM + architecture)
# âœ… num_worker_per_gpu: auto (optimized from CPU cores)
# âœ… accum_iter: auto (set based on VRAM constraints)
# âœ… pin_memory: auto (optimized for GPU training)

# AUTO-DETECTED TRAINING SETTINGS:
# âœ… base_lr: auto (optimized for architecture)
# âœ… total_iter: auto (set from architecture preset)
# âœ… warmup_iter: auto (optimized for stable training)
# âœ… optimizer settings: auto (AdamW with optimal params)
# âœ… EMA settings: auto (optimized for stability)

# AUTO-DETECTED AUTOMATION PARAMETERS:
train:
  # ğŸ¤– ALL AUTOMATIONS ENABLED - AUTO-CONFIGURED:
  training_automations:
    enabled: true

    # 1. Intelligent Learning Rate Scheduler
    IntelligentLearningRateScheduler:
      enabled: true
      # âœ… adaptation_threshold: auto (optimized for stability)
      # âœ… plateau_patience: auto (adjusted for hardware tier)
      # âœ… min_lr_factor/max_lr_factor: auto (safety bounds)
      # âœ… fallback scheduler: auto (cosine with optimal params)

    # 2. Dynamic Batch Size Optimizer
    DynamicBatchSizeOptimizer:
      enabled: true
      # âœ… target_vram_usage: auto (set from hardware tier)
      # âœ… safety_margin: auto (optimized for hardware)
      # âœ… batch_size limits: auto (based on VRAM capacity)
      # âœ… adjustment_frequency: auto (balanced for stability)

    # 3. Adaptive Gradient Clipping
    AdaptiveGradientClipping:
      enabled: true
      # âœ… initial_threshold: auto (optimized starting point)
      # âœ… min/max_threshold: auto (safety bounds)
      # âœ… adjustment_factor: auto (tuned for architecture)

    # 4. Intelligent Early Stopping
    IntelligentEarlyStopping:
      enabled: true
      # âœ… patience: auto (adjusted for training length)
      # âœ… monitor_metric: auto (best metric selection)
      # âœ… improvement_threshold: auto (tuned sensitivity)

  # âœ… Dynamic loss scheduling: auto (with auto-calibration)
  dynamic_loss_scheduling:
    enabled: true
    auto_calibrate: true

  # âœ… Loss configuration: auto (optimized for architecture)
  losses:
    - type: l1loss
      loss_weight: 1.0                       # âœ… Auto-balanced
    - type: ssimloss
      loss_weight: 0.05                      # âœ… Auto-balanced

# AUTO-DETECTED VALIDATION SETTINGS:
val:
  val_enabled: true
  # âœ… val_freq: auto (balanced for training length)
  save_img: false

  metrics_enabled: true
  metrics:
    psnr:
      type: calculate_psnr
      # âœ… crop_border: auto (optimized for scale factor)
    ssim:
      type: calculate_ssim
      # âœ… crop_border: auto (optimized for scale factor)

# AUTO-DETECTED LOGGING SETTINGS:
logger:
  # âœ… print_freq: auto (balanced logging frequency)
  save_checkpoint_freq: auto                  # âœ… Auto-set for training length
  save_checkpoint_format: safetensors        # âœ… Auto-selected format
  use_tb_logger: true                        # âœ… Auto-enabled

# AUTO-DETECTED PATH SETTINGS:
path:
  pretrain_network_g: ~                      # âœ… Auto-handled
  strict_load_g: true                       # âœ… Safe default
  resume_state: ~                           # âœ… Auto-handled


#########################################################################################
# SUMMARY: What you need to do vs what the framework does
#########################################################################################
#
# ğŸ“ YOU SPECIFY (3 lines):
# 1. scale: 2                              # Architecture scale
# 2. dataroot_gt/dataroot_lq (train)       # Training dataset paths
# 3. dataroot_gt/dataroot_lq (val)         # Validation dataset paths
# 4. type: paragonsr2_nano                 # Architecture name
#
# ğŸ¤– FRAMEWORK AUTO-DETECTS (everything else):
# âœ… Hardware detection (GPU, VRAM, CPU)
# âœ… Batch size optimization
# âœ… Learning rate scheduling
# âœ… All automation parameters
# âœ… Safety bounds and fallbacks
# âœ… Precision settings (AMP, BF16)
# âœ… Memory optimizations
# âœ… Validation frequency
# âœ… Checkpointing strategy
# âœ… 50+ other parameters
#
# RESULT: Same optimal training as manual config, but ZERO manual setup!
#########################################################################################

#########################################################################################
# OPTIONAL: Manual Overrides
#########################################################################################
# If you really want to override specific auto-detected values, you can:
#
# Example manual overrides:
# train:
#   optim_g:
#     lr: 1e-4                              # Override auto-detected LR
#   total_iter: 60000                       # Override training duration
#   training_automations:
#     DynamicBatchSizeOptimizer:
#       target_vram_usage: 0.90              # Override VRAM target
#
# But honestly, you probably don't need to override anything!
#########################################################################################
