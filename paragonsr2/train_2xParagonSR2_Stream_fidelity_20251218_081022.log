2025-12-18 08:10:22,850 INFO: ‚úÖ Logger: File logging activated: /home/phips/Documents/GitHub/traiNNer-redux/experiments/2xParagonSR2_Stream_fidelity/train_2xParagonSR2_Stream_fidelity_20251218_081022.log
2025-12-18 08:10:22,851 INFO: üìù Logger: File logging successfully added to existing logger
2025-12-18 08:10:23,544 INFO: [italic red]:rocket:  traiNNer-redux: good luck! :rocket:[/]
System Information: 
	Current GPU: 
		Name: NVIDIA GeForce RTX 3060
		Total VRAM: 11.63 GB
		Compute Capability: 8.6
		Multiprocessors: 28
	Storage:
		Free Space: 70.43 GB
Version Information: 
	traiNNer-redux: 
		Git commit: deafa14d48da737f4bdbe5b8baf8b1e543a05ab4 (dirty) ([green]Up to date[/green])
		Commit date: 2025-12-17 14:41:06 +0100
	PyTorch: 2.9.0+cu128
	TorchVision: 0.24.0+cu128
2025-12-18 08:10:23,548 DEBUG: #########################################################################################
# ParagonSR2 Stream Fidelity Config
# Optimized for highest PSNR/SSIM metrics with pure reconstruction (no sharpening)
# De-blocking focused variant for compressed video restoration
#########################################################################################
name: 2xParagonSR2_Stream_fidelity
scale: 2

# VRAM MANAGEMENT SYSTEM
auto_vram_management:
  enabled: true # Enable automatic VRAM management to maximize batch size dynamically
  target_vram_usage: 0.85 # Target 85% VRAM usage (leaves 15% headroon for system spikes)
  safety_margin: 0.05 # Additional 5% safety margin ensures reliability

use_amp: true # Automatic Mixed Precision (BF16) speeds up training and reduces VRAM
amp_bf16: true # BF16 is more stable than FP16 for training, protecting against NaN
use_channels_last: true # Always true for NVIDIA GPUs (Tensor Cores), works with compile
num_gpu: auto # Automatically detect and use all available GPUs
use_compile: false # Torch.compile enables kernel fusion, further boosting speed on 30-series+

datasets:
  train:
    name: CC0_147k_Train
    type: pairedimagedataset
    dataroot_gt: /home/phips/Documents/dataset/cc0/filtered_versions/cc0_c5_hr
    dataroot_lq: /home/phips/Documents/dataset/cc0/filtered_versions/cc0_c5_lr_x2

    # METRIC-OPTIMIZED PARAMETERS for RTX 3060 12GB (Auto-VRAM managed)
    lq_size: 256 # Large patch size (256 crop -> 512 HR) provides vital global context for the attention-free GateBlock
    batch_size_per_gpu: 24 # Initial batch size (managed by optimizer). 24 is a solid baseline for stable gradients
    num_worker_per_gpu: 8 # 8 workers balance CPU preprocessing speed with overhead
    accum_iter: 1 # Realtime updates preferred over accumulation for this batch size
    prefetch_mode: cpu # CPU prefetching reduces GPU VRAM pressure
    pin_memory: false # Disabling pin_memory prevents potential pinned memory fragmentation issues on consumer OS

    use_hflip: true # Data augmentation: Horizontal flip
    use_rot: true # Data augmentation: Rotation (90/180/270) increases effective dataset size 8x

  val:
    name: CC0_147k_Val
    type: pairedimagedataset
    dataroot_gt: /home/phips/Documents/dataset/cc0/val_hr
    dataroot_lq: /home/phips/Documents/dataset/cc0/val_lr_x2_bicubic_aa

network_g:
  type: paragonsr2_stream # The "Tiny" variant definition
  # FIDELITY OPTIMIZATION: Pure reconstruction for highest PSNR/SSIM
  upsampler_alpha: 0.0 # 0.0 = No sharpening. Crucial for PSNR/SSIM optimization as sharpening distorts pixel values
  use_checkpointing: true # Trade-off: Saves significant VRAM (allowing larger batch/patches) at cost of minor compute time. Worth it for 12GB cards.

path:
  pretrain_network_g: ~ # No pretrain path (training from scratch)
  strict_load_g: true

train:
  ema_decay: 0.999 # Standard EMA decay for stable model weights average
  ema_power: 0.75 # EMA power factor
  grad_clip: true # Prevents gradient explosion, critical for deep networks

  optim_g:
    type: AdamW # AdamW handles weight decay better than standard Adam
    lr: !!float 2e-4 # 2e-4 is the standard sweet spot for SR transformer/CNN hybrids
    weight_decay: !!float 1e-4 # Regularization to prevent overfitting on 147k dataset
    betas: [0.9, 0.99] # Standard AdamW betas

  scheduler:
    type: MultiStepLR # Simple, robust scheduler for fidelity training
    milestones: [40000, 50000, 55000] # Decays LR late in training to refine detailed convergence
    gamma: 0.5 # Halves the learning rate at each milestone

  total_iter: 200000 # 200k iterations is typically sufficient for convergence on this dataset size
  warmup_iter: 1000 # Warmup prevents unstable updates at the start

  # DYNAMIC LOSS SCHEDULING WITH AUTO-CALIBRATION
  dynamic_loss_scheduling:
    enabled: true # Automatically balances loss components (L1 vs MSSIM) during training

  # TRAINING AUTOMATIONS
  training_automations:
    enabled: true # Master enable for all automations
    intelligent_learning_rate_scheduler:
      enabled: true # Adapts LR based on loss plateauing

    DynamicBatchAndPatchSizeOptimizer:
      enabled: true # This is KEY for 12GB VRAM. It will push batch_size up to the limit automatically.
      target_vram_usage: 0.85
      safety_margin: 0.05
      adjustment_frequency: 100
      min_batch_size: 2
      max_batch_size: 64
      min_lq_size: 64
      max_lq_size: 256
      vram_history_size: 50

    IntelligentEarlyStopping:
      enabled: true # Saves resources if model stops improving
      patience: 10000
      min_improvement: 0.01
      monitor_metric: "val/psnr"
      convergence_threshold: 0.0001
      convergence_log_frequency: 500

    adaptive_gradient_clipping:
      enabled: true # Dynamic clipping threshold based on gradient history

  # OPTIMAL FIDELITY LOSS COMBINATION
  losses:
    - type: l1loss
      loss_weight: 1.0 # L1 is the primary driver for PSNR (pixel accuracy)
    - type: mssimloss
      loss_weight: 0.08 # Small MSSIM weight improves structural similarity (SSIM) without fighting L1 too much

val:
  val_enabled: true
  val_freq: 1000 # Frequent validation to track progress
  save_img: true

  metrics_enabled: true
  metrics:
    psnr:
      type: calculate_psnr
      crop_border: 4 # Standard crop for SR metrics
    ssim:
      type: calculate_ssim
      crop_border: 4

logger:
  print_freq: 100
  save_checkpoint_freq: 10000 # Save full models every 10k iters
  save_checkpoint_format: safetensors # Modern, safe format
  use_tb_logger: true

2025-12-18 08:10:23,552 INFO: Training with manual_seed=6512.
2025-12-18 08:10:23,554 INFO: Building Dataset CC0_147k_Train...
2025-12-18 08:10:23,943 INFO: Dataset [bold]PairedImageDataset[/bold] - CC0_147k_Train is built.
2025-12-18 08:10:23,945 INFO: Use cpu prefetch dataloader: num_prefetch_queue = 1
2025-12-18 08:10:23,946 INFO: Training statistics for [b]2xParagonSR2_Stream_fidelity[/b]:
	Number of train image pairs:      28,658	Dataset enlarge ratio:                 1
	Batch size per gpu:                   24	Accumulate iterations:                 1
	HR crop size:                        512	LR crop size:                        256
	World size (gpu number):               1	Require iter per epoch:            1,194
	Total epochs:                        168	Total iters:                     200,000
2025-12-18 08:10:23,948 INFO: Building Dataset CC0_147k_Val...
2025-12-18 08:10:23,951 INFO: Dataset [bold]PairedImageDataset[/bold] - CC0_147k_Val is built.
2025-12-18 08:10:23,952 INFO: Using custom collate function for channels_last memory format compatibility with pin_memory.
2025-12-18 08:10:23,952 INFO: Number of val images/folders in CC0_147k_Val: 70
2025-12-18 08:10:23,955 INFO: DEBUG: Registry keys after registration: ['intelligentlearningratescheduler', 'dynamicbatchandpatchsizeoptimizer', 'adaptivegradientclipping', 'intelligentearlystopping', 'adaptive_gradient_clipping', 'intelligent_learning_rate_scheduler', 'dynamic_batch_and_patch_size_optimizer', 'intelligent_early_stopping']
2025-12-18 08:10:23,957 INFO: Automation IntelligentLearningRateScheduler enabled with config: {'enabled': True}
2025-12-18 08:10:23,958 INFO: Initialized automation: intelligent_learning_rate_scheduler
2025-12-18 08:10:23,959 INFO: Automation DynamicBatchAndPatchSizeOptimizer enabled with config: {'enabled': True, 'target_vram_usage': 0.85, 'safety_margin': 0.05, 'adjustment_frequency': 100, 'min_batch_size': 2, 'max_batch_size': 64, 'min_lq_size': 64, 'max_lq_size': 256, 'vram_history_size': 50}
2025-12-18 08:10:23,960 INFO: Initialized automation: DynamicBatchAndPatchSizeOptimizer
2025-12-18 08:10:23,961 INFO: Automation IntelligentEarlyStopping enabled with config: {'enabled': True, 'patience': 10000, 'min_improvement': 0.01, 'monitor_metric': 'val/psnr', 'convergence_threshold': 0.0001, 'convergence_log_frequency': 500}
2025-12-18 08:10:23,962 INFO: Initialized automation: IntelligentEarlyStopping
2025-12-18 08:10:23,963 INFO: Automation AdaptiveGradientClipping enabled with config: {'enabled': True}
2025-12-18 08:10:23,964 INFO: ü§ñ AdaptiveGradientClipping: Autonomous mode enabled - auto-calibrating parameters based on detected architecture
2025-12-18 08:10:23,965 INFO: Initialized automation: adaptive_gradient_clipping
2025-12-18 08:10:23,976 INFO: Network [bold]ParagonSR2[/bold]({'upsampler_alpha': 0.0, 'use_checkpointing': True, 'scale': 2}) is created from [bold]traiNNer-redux[/bold].
2025-12-18 08:10:23,996 INFO: Using Automatic Mixed Precision (AMP) with fp32 and bf16.
2025-12-18 08:10:23,998 INFO: Using channels last memory format.
2025-12-18 08:10:23,998 INFO: Using Exponential Moving Average (EMA) with decay: 0.999.
2025-12-18 08:10:24,008 INFO: Gradient clipping is enabled.
2025-12-18 08:10:24,009 INFO: Loss [bold]L1Loss[/bold]({'loss_weight': 1.0}) is created.
2025-12-18 08:10:24,010 INFO: Loss [bold]MSSIMLoss[/bold]({'loss_weight': 0.08}) is created.
2025-12-18 08:10:24,011 INFO: Dynamic loss scheduling enabled with config: {'enabled': True}
2025-12-18 08:10:24,011 WARNING: Params base.sharp.h.weight will not be optimized.
2025-12-18 08:10:24,012 WARNING: Params base.sharp.v.weight will not be optimized.
2025-12-18 08:10:24,013 WARNING: Params base.blur.h.weight will not be optimized.
2025-12-18 08:10:24,013 WARNING: Params base.blur.v.weight will not be optimized.
2025-12-18 08:10:24,015 INFO: Optimizer [bold]AdamW[/bold]({'lr': 0.0002, 'weight_decay': 0.0001, 'betas': [0.9, 0.99]}) is created.
2025-12-18 08:10:24,016 INFO: Scheduler [bold]MultiStepLR[/bold]({'milestones': [40000, 50000, 55000], 'gamma': 0.5}) is created.
2025-12-18 08:10:24,017 INFO: Model [bold]SRModel[/bold] is created.
2025-12-18 08:10:24,389 INFO: Dynamic dataloader initialized with batch_size: 24 (original: None)
2025-12-18 08:10:24,390 INFO: Automation DynamicBatchAndPatchSizeOptimizer: Dynamic wrappers set - Dataloader: True, Dataset: True
2025-12-18 08:10:24,392 INFO: Automation DynamicBatchAndPatchSizeOptimizer: Starting VRAM monitoring period (adjustment_frequency: 100 iterations). Initial Peak VRAM: 0.000 (0.0%)
2025-12-18 08:10:24,393 INFO: BaseModel: Dynamic wrappers set for VRAM management - Dataloader: True, Dataset: True
2025-12-18 08:10:24,394 INFO: Automation DynamicBatchAndPatchSizeOptimizer: Parameters initialized - Batch: 24, LQ: 256
2025-12-18 08:10:24,394 INFO: ‚úÖ VRAM Automation verification - Config batch: 24, lq: 256, Automation batch: 24, Automation lq: 256
2025-12-18 08:10:24,395 INFO: üî• Initial VRAM usage: 0.000 (0.0%)
2025-12-18 08:10:24,396 INFO: Dynamic VRAM management initialized - Batch: 24, LQ: 256, Dynamic Wrappers: Enabled
2025-12-18 08:10:24,397 INFO: ================================================================================
2025-12-18 08:10:24,398 INFO: üöÄ ENHANCED TRAINING LOGGING ENABLED
2025-12-18 08:10:24,398 INFO: ================================================================================
2025-12-18 08:10:24,399 INFO: üìä Network: paragonsr2_stream (scale: 2x)
2025-12-18 08:10:24,400 INFO: ‚öôÔ∏è  Config: patch: 256, scale: 2x
2025-12-18 08:10:24,401 INFO: üéØ Losses: l1loss(1.00e+00), mssimloss(8.00e-02)
2025-12-18 08:10:24,401 INFO: ü§ñ Automations: Intelligent Learning Rate Scheduler, Dynamicbatchandpatchsizeoptimizer, Intelligentearlystopping, Adaptive Gradient Clipping (4 enabled)
2025-12-18 08:10:24,402 INFO: ================================================================================
2025-12-18 08:10:24,509 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0001 (0.00GB), peak: 0.0001 (0.00GB/12.49GB), target: 0.85
2025-12-18 08:10:24,510 INFO: üöÄ Early VRAM monitoring - Initialization phase (iterations 0-99): Monitoring VRAM without adjustments, batch: 24, LQ: 256
2025-12-18 08:10:24,512 INFO: Start training from epoch: 0, iter: 0.
2025-12-18 08:12:14,251 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0121 (0.15GB), peak: 0.4572 (5.71GB/12.49GB), target: 0.85
2025-12-18 08:12:58,041 INFO: Dynamic loss scheduler baseline established at iteration 100. Baseline loss values: {'l_g_l1': 0.03562122583389282, 'l_g_mssim': 0.03837871551513672}
2025-12-18 08:12:58,613 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0121 (0.15GB), peak: 0.4572 (5.71GB/12.49GB), target: 0.85
2025-12-18 08:12:58,614 INFO: VRAM DEBUG: Peak usage: 0.457, Target: 0.850, Safety margin: 0.050, Current batch: 24, Current lq_size: 256
2025-12-18 08:12:58,615 INFO: üîÑ VRAM OPTIMIZATION (PEAK-BASED): Remaining memory 0.393 (39.3%), suggesting batch_size increase of +8 (24 ‚Üí 32)
2025-12-18 08:12:58,616 INFO: üéØ VRAM OPTIMIZATION DECISION (PEAK-BASED): Peak VRAM: 0.457 (45.7%), Batch adjustment: +8, LQ adjustment: +0
2025-12-18 08:12:58,617 INFO: Suggested adjustments - Batch: +8, LQ: +0
2025-12-18 08:12:58,618 INFO: Automation suggests adjustments - Batch size: +8, LQ size: +0
2025-12-18 08:12:58,619 INFO: Automation DynamicBatchAndPatchSizeOptimizer: Parameters initialized - Batch: 32, LQ: 256
2025-12-18 08:12:58,620 DEBUG: Batch size updated to: 32
2025-12-18 08:12:58,620 INFO: Dynamic dataloader: batch_size ACTUALLY CHANGED 24 ‚Üí 32 (VRAM will now reflect this change)
2025-12-18 08:12:58,621 INFO: Batch size adjusted: 24 ‚Üí 32
2025-12-18 08:12:58,626 INFO: [epoch:   0, iter:     100, lr:(2.000e-05)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.649 it/s, eta: 2 days, 0:15:29] [VRAM: 0.14 GB, peak: 0.14 GB, cached: 3.83 GB] [total: 3.382e-02, content: 3.115e-02] [vram: low] [l_g_l1: 3.1155e-02, l_g_mssim: 2.6683e-03, l_g_total: 3.3823e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 1.0000e+00 grad_norm_g: 9.0771e-03 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 1.4062e-01 
2025-12-18 08:15:17,966 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0156 (0.20GB), peak: 0.6091 (7.61GB/12.49GB), target: 0.85
2025-12-18 08:16:18,307 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0156 (0.20GB), peak: 0.6091 (7.61GB/12.49GB), target: 0.85
2025-12-18 08:16:18,310 INFO: [epoch:   0, iter:     200, lr:(4.000e-05)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.565 it/s, eta: 3 days, 7:22:25] [VRAM: 0.18 GB, peak: 7.09 GB, cached: 4.29 GB] [total: 3.383e-02, content: 3.120e-02] [vram: low] [l_g_l1: 3.1196e-02, l_g_mssim: 2.6315e-03, l_g_total: 3.3828e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 1.0001e+00 grad_norm_g: 4.5974e-03 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 1.8164e-01 
2025-12-18 08:16:19,540 INFO: üîÑ VRAM OPTIMIZATION (PEAK-BASED): Remaining memory 0.241 (24.1%), suggesting batch_size increase of +8 (32 ‚Üí 40)
2025-12-18 08:16:19,541 INFO: üéØ VRAM OPTIMIZATION DECISION (PEAK-BASED): Peak VRAM: 0.609 (60.9%), Batch adjustment: +8, LQ adjustment: +0
2025-12-18 08:16:19,542 INFO: Suggested adjustments - Batch: +8, LQ: +0
2025-12-18 08:16:19,543 INFO: Automation suggests adjustments - Batch size: +8, LQ size: +0
2025-12-18 08:16:19,544 INFO: Automation DynamicBatchAndPatchSizeOptimizer: Parameters initialized - Batch: 40, LQ: 256
2025-12-18 08:16:19,545 DEBUG: Batch size updated to: 40
2025-12-18 08:16:19,545 INFO: Dynamic dataloader: batch_size ACTUALLY CHANGED 32 ‚Üí 40 (VRAM will now reflect this change)
2025-12-18 08:16:19,546 INFO: Batch size adjusted: 32 ‚Üí 40
2025-12-18 08:19:27,764 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0191 (0.24GB), peak: 0.7609 (9.51GB/12.49GB), target: 0.85
2025-12-18 08:20:42,551 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0191 (0.24GB), peak: 0.7609 (9.51GB/12.49GB), target: 0.85
2025-12-18 08:20:42,555 INFO: [epoch:   0, iter:     300, lr:(6.000e-05)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.376 it/s, eta: 4 days, 5:40:31] [VRAM: 0.22 GB, peak: 8.85 GB, cached: 6.64 GB] [total: 3.187e-02, content: 2.970e-02] [vram: low] [l_g_l1: 2.9698e-02, l_g_mssim: 2.1741e-03, l_g_total: 3.1872e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 1.0002e+00 grad_norm_g: 1.5317e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.2265e-01 
2025-12-18 08:20:45,555 INFO: üîÑ VRAM OPTIMIZATION (PEAK-BASED): Remaining memory 0.089 (8.9%), suggesting batch_size increase of +2 (40 ‚Üí 42)
2025-12-18 08:20:45,556 INFO: üéØ VRAM OPTIMIZATION DECISION (PEAK-BASED): Peak VRAM: 0.761 (76.1%), Batch adjustment: +2, LQ adjustment: +0
2025-12-18 08:20:45,557 INFO: Suggested adjustments - Batch: +2, LQ: +0
2025-12-18 08:20:45,557 INFO: Automation suggests adjustments - Batch size: +2, LQ size: +0
2025-12-18 08:20:45,558 INFO: Automation DynamicBatchAndPatchSizeOptimizer: Parameters initialized - Batch: 42, LQ: 256
2025-12-18 08:20:45,559 DEBUG: Batch size updated to: 42
2025-12-18 08:20:45,559 INFO: Dynamic dataloader: batch_size ACTUALLY CHANGED 40 ‚Üí 42 (VRAM will now reflect this change)
2025-12-18 08:20:45,560 INFO: Batch size adjusted: 40 ‚Üí 42
2025-12-18 08:24:01,470 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0200 (0.25GB), peak: 0.7989 (9.98GB/12.49GB), target: 0.85
2025-12-18 08:25:20,541 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0200 (0.25GB), peak: 0.7989 (9.98GB/12.49GB), target: 0.85
2025-12-18 08:25:20,546 INFO: [epoch:   0, iter:     400, lr:(8.000e-05)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.368 it/s, eta: 4 days, 18:43:04] [VRAM: 0.23 GB, peak: 9.29 GB, cached: 5.62 GB] [total: 2.745e-02, content: 2.616e-02] [vram: low] [l_g_l1: 2.6159e-02, l_g_mssim: 1.2949e-03, l_g_total: 2.7454e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 1.0004e+00 grad_norm_g: 2.0245e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.3291e-01 
2025-12-18 08:25:25,262 INFO: üîÑ VRAM OPTIMIZATION (PEAK-BASED): Remaining memory 0.051 (5.1%), suggesting batch_size increase of +2 (42 ‚Üí 44)
2025-12-18 08:25:25,263 INFO: üéØ VRAM OPTIMIZATION DECISION (PEAK-BASED): Peak VRAM: 0.799 (79.9%), Batch adjustment: +2, LQ adjustment: +0
2025-12-18 08:25:25,264 INFO: Suggested adjustments - Batch: +2, LQ: +0
2025-12-18 08:25:25,265 INFO: Automation suggests adjustments - Batch size: +2, LQ size: +0
2025-12-18 08:25:25,266 INFO: Automation DynamicBatchAndPatchSizeOptimizer: Parameters initialized - Batch: 44, LQ: 256
2025-12-18 08:25:25,267 DEBUG: Batch size updated to: 44
2025-12-18 08:25:25,267 INFO: Dynamic dataloader: batch_size ACTUALLY CHANGED 42 ‚Üí 44 (VRAM will now reflect this change)
2025-12-18 08:25:25,268 INFO: Batch size adjusted: 42 ‚Üí 44
2025-12-18 08:28:52,632 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 08:30:14,595 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 08:30:14,597 INFO: üîç VRAM DEBUG (iter 500): VRAM: 0.021 (2.1%), Target: 0.850 (85.0%), Current batch: 44, Current lq: 256, Min batch: 2, Max batch: 64, Min lq: 64, Max lq: 256
2025-12-18 08:30:14,601 INFO: [epoch:   0, iter:     500, lr:(1.000e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.337 it/s, eta: 5 days, 4:18:00] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 7.29 GB] [total: 2.331e-02, content: 2.265e-02] [vram: low] [l_g_l1: 2.2651e-02, l_g_mssim: 6.5692e-04, l_g_total: 2.3308e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 1.0002e+00 grad_norm_g: 1.4590e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 08:31:36,525 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 08:31:36,526 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 08:32:58,522 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 08:32:58,524 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 08:32:58,529 INFO: [epoch:   0, iter:     600, lr:(1.200e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.435 it/s, eta: 4 days, 22:40:25] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 7.29 GB] [total: 2.138e-02, content: 2.094e-02] [vram: low] [l_g_l1: 2.0938e-02, l_g_mssim: 4.4478e-04, l_g_total: 2.1383e-02] dynamic_weight_l_g_l1: 1.0001e+00 dynamic_weight_l_g_mssim: 1.0001e+00 grad_norm_g: 1.4285e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 08:34:20,650 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 08:34:20,653 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 08:35:42,766 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 08:35:42,767 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 08:35:42,772 INFO: [epoch:   0, iter:     700, lr:(1.400e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.609 it/s, eta: 4 days, 18:39:51] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 7.29 GB] [total: 2.061e-02, content: 2.022e-02] [vram: low] [l_g_l1: 2.0222e-02, l_g_mssim: 3.8412e-04, l_g_total: 2.0606e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 1.0001e+00 grad_norm_g: 1.9969e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 08:37:19,298 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 08:37:19,299 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 08:38:41,034 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 08:38:41,035 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 08:38:41,040 INFO: [epoch:   1, iter:     800, lr:(1.600e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.584 it/s, eta: 4 days, 16:36:48] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 7.22 GB] [total: 2.012e-02, content: 1.977e-02] [vram: low] [l_g_l1: 1.9765e-02, l_g_mssim: 3.5602e-04, l_g_total: 2.0122e-02] dynamic_weight_l_g_l1: 9.9996e-01 dynamic_weight_l_g_mssim: 1.0000e+00 grad_norm_g: 4.2295e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 08:40:02,886 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 08:40:02,889 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 08:41:25,034 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 08:41:25,035 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 08:41:25,039 INFO: [epoch:   1, iter:     900, lr:(1.800e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.610 it/s, eta: 4 days, 14:07:52] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 7.22 GB] [total: 1.999e-02, content: 1.964e-02] [vram: low] [l_g_l1: 1.9644e-02, l_g_mssim: 3.4600e-04, l_g_total: 1.9990e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 1.0000e+00 grad_norm_g: 4.8479e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 08:42:47,090 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 08:42:47,091 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 08:44:09,138 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 08:44:09,139 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 08:44:09,141 INFO: üîç VRAM DEBUG (iter 1000): VRAM: 0.021 (2.1%), Target: 0.850 (85.0%), Current batch: 44, Current lq: 256, Min batch: 2, Max batch: 64, Min lq: 64, Max lq: 256
2025-12-18 08:44:09,145 INFO: [epoch:   1, iter:   1,000, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.610 it/s, eta: 4 days, 12:08:30] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 7.22 GB] [total: 1.980e-02, content: 1.946e-02] [vram: low] [l_g_l1: 1.9461e-02, l_g_mssim: 3.3914e-04, l_g_total: 1.9801e-02] dynamic_weight_l_g_l1: 9.9997e-01 dynamic_weight_l_g_mssim: 1.0000e+00 grad_norm_g: 6.2863e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 08:44:09,150 INFO: Saving 70 validation images to [link=file:////home/phips/Documents/GitHub/traiNNer-redux/experiments/2xParagonSR2_Stream_fidelity/visualization]visualization folder[/link].
2025-12-18 08:44:46,179 INFO: Validation CC0_147k_Val
	 # psnr : 34.0643	Best: 34.0643 @     1,000 iter
	 # ssim :  0.9386	Best:  0.9386 @     1,000 iter

2025-12-18 08:46:08,179 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 08:46:08,226 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 08:47:30,263 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 08:47:30,265 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 08:47:30,268 INFO: [epoch:   1, iter:   1,100, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.609 it/s, eta: 4 days, 12:21:45] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.970e-02, content: 1.937e-02] [vram: low] [l_g_l1: 1.9365e-02, l_g_mssim: 3.3581e-04, l_g_total: 1.9701e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 9.9999e-01 grad_norm_g: 5.3150e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 08:48:52,321 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 08:48:52,322 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 08:50:14,420 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 08:50:14,468 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 08:50:14,480 INFO: [epoch:   1, iter:   1,200, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.609 it/s, eta: 4 days, 10:50:24] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.976e-02, content: 1.942e-02] [vram: low] [l_g_l1: 1.9423e-02, l_g_mssim: 3.3961e-04, l_g_total: 1.9763e-02] dynamic_weight_l_g_l1: 1.0001e+00 dynamic_weight_l_g_mssim: 1.0000e+00 grad_norm_g: 5.8028e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 08:51:36,582 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 08:51:36,583 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 08:52:58,679 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 08:52:58,680 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 08:52:58,684 INFO: [epoch:   1, iter:   1,300, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.609 it/s, eta: 4 days, 9:32:42] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.983e-02, content: 1.949e-02] [vram: low] [l_g_l1: 1.9490e-02, l_g_mssim: 3.3907e-04, l_g_total: 1.9829e-02] dynamic_weight_l_g_l1: 9.9999e-01 dynamic_weight_l_g_mssim: 9.9998e-01 grad_norm_g: 4.8154e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 08:54:20,954 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 08:54:20,955 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 08:56:18,529 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 08:56:18,531 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 08:56:18,536 INFO: [epoch:   2, iter:   1,400, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.548 it/s, eta: 4 days, 9:49:53] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.954e-02, content: 1.920e-02] [vram: low] [l_g_l1: 1.9202e-02, l_g_mssim: 3.3609e-04, l_g_total: 1.9539e-02] dynamic_weight_l_g_l1: 9.9996e-01 dynamic_weight_l_g_mssim: 9.9995e-01 grad_norm_g: 4.0187e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 08:57:40,593 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 08:57:40,594 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 08:59:02,675 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 08:59:02,677 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 08:59:02,678 INFO: üîç VRAM DEBUG (iter 1500): VRAM: 0.021 (2.1%), Target: 0.850 (85.0%), Current batch: 44, Current lq: 256, Min batch: 2, Max batch: 64, Min lq: 64, Max lq: 256
2025-12-18 08:59:02,681 INFO: [epoch:   2, iter:   1,500, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.609 it/s, eta: 4 days, 8:45:39] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.971e-02, content: 1.937e-02] [vram: low] [l_g_l1: 1.9375e-02, l_g_mssim: 3.3632e-04, l_g_total: 1.9711e-02] dynamic_weight_l_g_l1: 9.9999e-01 dynamic_weight_l_g_mssim: 9.9999e-01 grad_norm_g: 4.5993e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 09:00:24,891 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:00:24,894 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:01:47,211 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:01:47,212 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:01:47,217 INFO: [epoch:   2, iter:   1,600, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.608 it/s, eta: 4 days, 7:49:54] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.960e-02, content: 1.926e-02] [vram: low] [l_g_l1: 1.9261e-02, l_g_mssim: 3.3703e-04, l_g_total: 1.9598e-02] dynamic_weight_l_g_l1: 9.9998e-01 dynamic_weight_l_g_mssim: 9.9997e-01 grad_norm_g: 6.3292e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 09:03:09,537 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:03:09,538 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:04:31,873 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:04:31,874 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:04:31,879 INFO: [epoch:   2, iter:   1,700, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.607 it/s, eta: 4 days, 7:00:37] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.933e-02, content: 1.900e-02] [vram: low] [l_g_l1: 1.8998e-02, l_g_mssim: 3.3378e-04, l_g_total: 1.9332e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 1.0000e+00 grad_norm_g: 2.9695e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 09:05:54,174 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:05:54,178 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:07:16,463 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:07:16,464 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:07:16,515 INFO: [epoch:   2, iter:   1,800, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.607 it/s, eta: 4 days, 6:16:23] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.939e-02, content: 1.905e-02] [vram: low] [l_g_l1: 1.9053e-02, l_g_mssim: 3.3352e-04, l_g_total: 1.9387e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 1.0000e+00 grad_norm_g: 4.3621e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 09:08:38,829 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:08:38,830 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:10:01,152 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:10:01,155 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:10:01,159 INFO: [epoch:   2, iter:   1,900, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.607 it/s, eta: 4 days, 5:36:41] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.935e-02, content: 1.902e-02] [vram: low] [l_g_l1: 1.9022e-02, l_g_mssim: 3.3300e-04, l_g_total: 1.9355e-02] dynamic_weight_l_g_l1: 9.9999e-01 dynamic_weight_l_g_mssim: 9.9999e-01 grad_norm_g: 2.3354e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 09:11:23,509 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:11:23,510 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:12:45,930 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:12:45,931 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:12:45,932 INFO: üîç VRAM DEBUG (iter 2000): VRAM: 0.021 (2.1%), Target: 0.850 (85.0%), Current batch: 44, Current lq: 256, Min batch: 2, Max batch: 64, Min lq: 64, Max lq: 256
2025-12-18 09:12:45,936 INFO: [epoch:   2, iter:   2,000, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.607 it/s, eta: 4 days, 5:00:50] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.929e-02, content: 1.895e-02] [vram: low] [l_g_l1: 1.8952e-02, l_g_mssim: 3.3337e-04, l_g_total: 1.9286e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 9.9999e-01 grad_norm_g: 3.0935e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 09:12:45,940 INFO: Saving 70 validation images to [link=file:////home/phips/Documents/GitHub/traiNNer-redux/experiments/2xParagonSR2_Stream_fidelity/visualization]visualization folder[/link].
2025-12-18 09:13:19,798 INFO: Validation CC0_147k_Val
	 # psnr : 34.2625	Best: 34.2625 @     2,000 iter
	 # ssim :  0.9408	Best:  0.9408 @     2,000 iter

2025-12-18 09:14:41,429 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:14:41,430 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:16:03,595 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:16:03,597 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:16:03,600 INFO: [epoch:   3, iter:   2,100, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.610 it/s, eta: 4 days, 5:19:45] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.914e-02, content: 1.881e-02] [vram: low] [l_g_l1: 1.8807e-02, l_g_mssim: 3.3001e-04, l_g_total: 1.9137e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 9.9999e-01 grad_norm_g: 2.8916e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 09:17:25,857 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:17:25,861 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:18:48,246 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:18:48,247 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:18:48,254 INFO: [epoch:   3, iter:   2,200, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.609 it/s, eta: 4 days, 4:47:12] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.926e-02, content: 1.893e-02] [vram: low] [l_g_l1: 1.8926e-02, l_g_mssim: 3.3400e-04, l_g_total: 1.9260e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 1.0000e+00 grad_norm_g: 3.6081e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 09:20:10,528 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:20:10,529 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:21:32,913 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:21:32,916 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:21:32,920 INFO: [epoch:   3, iter:   2,300, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.607 it/s, eta: 4 days, 4:17:16] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.938e-02, content: 1.904e-02] [vram: low] [l_g_l1: 1.9044e-02, l_g_mssim: 3.3322e-04, l_g_total: 1.9377e-02] dynamic_weight_l_g_l1: 9.9999e-01 dynamic_weight_l_g_mssim: 9.9999e-01 grad_norm_g: 2.3538e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 09:22:55,187 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:22:55,188 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:24:17,383 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:24:17,385 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:24:17,389 INFO: [epoch:   3, iter:   2,400, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.608 it/s, eta: 4 days, 3:49:20] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.933e-02, content: 1.899e-02] [vram: low] [l_g_l1: 1.8994e-02, l_g_mssim: 3.3213e-04, l_g_total: 1.9326e-02] dynamic_weight_l_g_l1: 9.9999e-01 dynamic_weight_l_g_mssim: 9.9996e-01 grad_norm_g: 2.6766e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 09:25:39,787 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:25:39,810 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:27:02,256 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:27:02,258 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:27:02,259 INFO: üîç VRAM DEBUG (iter 2500): VRAM: 0.021 (2.1%), Target: 0.850 (85.0%), Current batch: 44, Current lq: 256, Min batch: 2, Max batch: 64, Min lq: 64, Max lq: 256
2025-12-18 09:27:02,263 INFO: [epoch:   3, iter:   2,500, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.607 it/s, eta: 4 days, 3:23:56] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.927e-02, content: 1.894e-02] [vram: low] [l_g_l1: 1.8941e-02, l_g_mssim: 3.3307e-04, l_g_total: 1.9275e-02] dynamic_weight_l_g_l1: 9.9997e-01 dynamic_weight_l_g_mssim: 1.0000e+00 grad_norm_g: 3.4734e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 09:28:24,660 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:28:24,662 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:29:47,048 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:29:47,071 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:29:47,085 INFO: [epoch:   3, iter:   2,600, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.607 it/s, eta: 4 days, 3:00:13] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.902e-02, content: 1.869e-02] [vram: low] [l_g_l1: 1.8692e-02, l_g_mssim: 3.2814e-04, l_g_total: 1.9020e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 9.9997e-01 grad_norm_g: 2.2000e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 09:31:09,456 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:31:09,457 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:32:31,213 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:32:31,214 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:32:31,218 INFO: [epoch:   4, iter:   2,700, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.610 it/s, eta: 4 days, 2:37:13] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.922e-02, content: 1.888e-02] [vram: low] [l_g_l1: 1.8884e-02, l_g_mssim: 3.3079e-04, l_g_total: 1.9215e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 1.0000e+00 grad_norm_g: 1.7032e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 09:33:53,447 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:33:53,448 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:35:15,793 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:35:15,794 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:35:15,798 INFO: [epoch:   4, iter:   2,800, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.609 it/s, eta: 4 days, 2:16:11] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.911e-02, content: 1.878e-02] [vram: low] [l_g_l1: 1.8775e-02, l_g_mssim: 3.3110e-04, l_g_total: 1.9106e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 9.9997e-01 grad_norm_g: 3.0608e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 09:36:38,176 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:36:38,180 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:38:00,505 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:38:00,506 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:38:00,509 INFO: [epoch:   4, iter:   2,900, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.607 it/s, eta: 4 days, 1:56:34] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.927e-02, content: 1.894e-02] [vram: low] [l_g_l1: 1.8942e-02, l_g_mssim: 3.3214e-04, l_g_total: 1.9274e-02] dynamic_weight_l_g_l1: 9.9995e-01 dynamic_weight_l_g_mssim: 1.0000e+00 grad_norm_g: 2.1002e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 09:39:22,663 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:39:22,665 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:40:44,946 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:40:44,949 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:40:44,950 INFO: üîç VRAM DEBUG (iter 3000): VRAM: 0.021 (2.1%), Target: 0.850 (85.0%), Current batch: 44, Current lq: 256, Min batch: 2, Max batch: 64, Min lq: 64, Max lq: 256
2025-12-18 09:40:44,954 INFO: [epoch:   4, iter:   3,000, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.608 it/s, eta: 4 days, 1:37:47] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.920e-02, content: 1.887e-02] [vram: low] [l_g_l1: 1.8869e-02, l_g_mssim: 3.2921e-04, l_g_total: 1.9198e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 1.0000e+00 grad_norm_g: 1.5741e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 09:40:44,959 INFO: Saving 70 validation images to [link=file:////home/phips/Documents/GitHub/traiNNer-redux/experiments/2xParagonSR2_Stream_fidelity/visualization]visualization folder[/link].
2025-12-18 09:41:18,222 INFO: Validation CC0_147k_Val
	 # psnr : 34.3951	Best: 34.3951 @     3,000 iter
	 # ssim :  0.9430	Best:  0.9430 @     3,000 iter

2025-12-18 09:42:40,208 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:42:40,209 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:44:02,237 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:44:02,238 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:44:02,242 INFO: [epoch:   4, iter:   3,100, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.609 it/s, eta: 4 days, 1:54:47] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.892e-02, content: 1.859e-02] [vram: low] [l_g_l1: 1.8592e-02, l_g_mssim: 3.2902e-04, l_g_total: 1.8921e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 1.0000e+00 grad_norm_g: 1.9354e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 09:45:24,446 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:45:24,449 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:46:46,737 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:46:46,738 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:46:46,777 INFO: [epoch:   4, iter:   3,200, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.609 it/s, eta: 4 days, 1:36:55] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.896e-02, content: 1.863e-02] [vram: low] [l_g_l1: 1.8630e-02, l_g_mssim: 3.2734e-04, l_g_total: 1.8957e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 1.0000e+00 grad_norm_g: 1.4148e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 09:48:08,997 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:48:08,999 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:49:31,000 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:49:31,004 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:49:31,007 INFO: [epoch:   4, iter:   3,300, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.609 it/s, eta: 4 days, 1:19:45] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.916e-02, content: 1.883e-02] [vram: low] [l_g_l1: 1.8833e-02, l_g_mssim: 3.3025e-04, l_g_total: 1.9164e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 9.9999e-01 grad_norm_g: 1.5556e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 09:50:52,547 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:50:52,548 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:52:14,826 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:52:14,827 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:52:14,868 INFO: [epoch:   5, iter:   3,400, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.610 it/s, eta: 4 days, 1:02:59] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.919e-02, content: 1.886e-02] [vram: low] [l_g_l1: 1.8855e-02, l_g_mssim: 3.3313e-04, l_g_total: 1.9189e-02] dynamic_weight_l_g_l1: 9.9999e-01 dynamic_weight_l_g_mssim: 1.0000e+00 grad_norm_g: 1.5627e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 09:53:37,206 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:53:37,207 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:54:59,449 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:54:59,450 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:54:59,451 INFO: üîç VRAM DEBUG (iter 3500): VRAM: 0.021 (2.1%), Target: 0.850 (85.0%), Current batch: 44, Current lq: 256, Min batch: 2, Max batch: 64, Min lq: 64, Max lq: 256
2025-12-18 09:54:59,454 INFO: [epoch:   5, iter:   3,500, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.608 it/s, eta: 4 days, 0:47:47] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.902e-02, content: 1.869e-02] [vram: low] [l_g_l1: 1.8691e-02, l_g_mssim: 3.2888e-04, l_g_total: 1.9020e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 1.0000e+00 grad_norm_g: 1.3661e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 09:56:21,720 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:56:21,724 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:57:44,063 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:57:44,064 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 09:57:44,068 INFO: [epoch:   5, iter:   3,600, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.608 it/s, eta: 4 days, 0:33:16] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.895e-02, content: 1.862e-02] [vram: low] [l_g_l1: 1.8622e-02, l_g_mssim: 3.2578e-04, l_g_total: 1.8948e-02] dynamic_weight_l_g_l1: 9.9999e-01 dynamic_weight_l_g_mssim: 1.0000e+00 grad_norm_g: 1.2099e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 09:59:06,438 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 09:59:06,439 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 10:00:28,743 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 10:00:28,746 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 10:00:28,749 INFO: [epoch:   5, iter:   3,700, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.607 it/s, eta: 4 days, 0:19:26] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.897e-02, content: 1.865e-02] [vram: low] [l_g_l1: 1.8647e-02, l_g_mssim: 3.2686e-04, l_g_total: 1.8974e-02] dynamic_weight_l_g_l1: 9.9999e-01 dynamic_weight_l_g_mssim: 9.9996e-01 grad_norm_g: 1.4199e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 10:01:50,972 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 10:01:50,973 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 10:03:13,198 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 10:03:13,199 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 10:03:13,204 INFO: [epoch:   5, iter:   3,800, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.608 it/s, eta: 4 days, 0:06:00] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.927e-02, content: 1.894e-02] [vram: low] [l_g_l1: 1.8939e-02, l_g_mssim: 3.3293e-04, l_g_total: 1.9272e-02] dynamic_weight_l_g_l1: 9.9995e-01 dynamic_weight_l_g_mssim: 9.9996e-01 grad_norm_g: 9.0682e-03 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 10:04:35,410 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 10:04:35,411 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 10:05:57,672 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 10:05:57,676 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 10:05:57,679 INFO: [epoch:   5, iter:   3,900, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.608 it/s, eta: 3 days, 23:53:08] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.901e-02, content: 1.868e-02] [vram: low] [l_g_l1: 1.8678e-02, l_g_mssim: 3.3055e-04, l_g_total: 1.9009e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 1.0000e+00 grad_norm_g: 1.0621e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 10:07:19,954 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 10:07:19,956 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 10:08:41,697 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 10:08:41,698 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 10:08:41,701 INFO: üîç VRAM DEBUG (iter 4000): VRAM: 0.021 (2.1%), Target: 0.850 (85.0%), Current batch: 44, Current lq: 256, Min batch: 2, Max batch: 64, Min lq: 64, Max lq: 256
2025-12-18 10:08:41,705 INFO: [epoch:   6, iter:   4,000, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.609 it/s, eta: 3 days, 23:40:24] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.882e-02, content: 1.849e-02] [vram: low] [l_g_l1: 1.8493e-02, l_g_mssim: 3.2621e-04, l_g_total: 1.8819e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 1.0000e+00 grad_norm_g: 1.0750e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 10:08:41,710 INFO: Saving 70 validation images to [link=file:////home/phips/Documents/GitHub/traiNNer-redux/experiments/2xParagonSR2_Stream_fidelity/visualization]visualization folder[/link].
2025-12-18 10:09:15,056 INFO: Validation CC0_147k_Val
	 # psnr : 34.4705	Best: 34.4705 @     4,000 iter
	 # ssim :  0.9444	Best:  0.9444 @     4,000 iter

2025-12-18 10:10:36,888 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 10:10:36,940 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 10:11:58,971 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 10:11:58,972 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 10:11:58,976 INFO: [epoch:   6, iter:   4,100, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.610 it/s, eta: 3 days, 23:54:37] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.904e-02, content: 1.871e-02] [vram: low] [l_g_l1: 1.8707e-02, l_g_mssim: 3.2920e-04, l_g_total: 1.9037e-02] dynamic_weight_l_g_l1: 1.0001e+00 dynamic_weight_l_g_mssim: 1.0000e+00 grad_norm_g: 1.0413e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 10:13:21,099 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 10:13:21,100 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 10:14:43,203 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 10:14:43,205 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 10:14:43,210 INFO: [epoch:   6, iter:   4,200, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.609 it/s, eta: 3 days, 23:42:21] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.908e-02, content: 1.875e-02] [vram: low] [l_g_l1: 1.8748e-02, l_g_mssim: 3.3135e-04, l_g_total: 1.9079e-02] dynamic_weight_l_g_l1: 9.9999e-01 dynamic_weight_l_g_mssim: 1.0000e+00 grad_norm_g: 7.9811e-03 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 10:16:05,349 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 10:16:05,352 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 10:17:27,530 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 10:17:27,531 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 10:17:27,535 INFO: [epoch:   6, iter:   4,300, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.609 it/s, eta: 3 days, 23:30:35] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.892e-02, content: 1.859e-02] [vram: low] [l_g_l1: 1.8592e-02, l_g_mssim: 3.2771e-04, l_g_total: 1.8919e-02] dynamic_weight_l_g_l1: 1.0000e+00 dynamic_weight_l_g_mssim: 1.0000e+00 grad_norm_g: 1.1254e-02 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 10:18:49,629 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 10:18:49,630 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 10:20:11,804 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 10:20:11,808 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 10:20:11,811 INFO: [epoch:   6, iter:   4,400, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.609 it/s, eta: 3 days, 23:19:12] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.910e-02, content: 1.877e-02] [vram: low] [l_g_l1: 1.8773e-02, l_g_mssim: 3.3162e-04, l_g_total: 1.9104e-02] dynamic_weight_l_g_l1: 9.9999e-01 dynamic_weight_l_g_mssim: 9.9995e-01 grad_norm_g: 7.2201e-03 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 10:21:33,878 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 10:21:33,879 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 10:22:55,819 INFO: Automation DynamicBatchAndPatchSizeOptimizer: VRAM usage 0.0209 (0.26GB), peak: 0.8368 (10.45GB/12.49GB), target: 0.85
2025-12-18 10:22:55,820 INFO: VRAM DEBUG: Peak usage: 0.837, Target: 0.850, Safety margin: 0.050, Current batch: 44, Current lq_size: 256
2025-12-18 10:22:55,821 INFO: üîç VRAM DEBUG (iter 4500): VRAM: 0.021 (2.1%), Target: 0.850 (85.0%), Current batch: 44, Current lq: 256, Min batch: 2, Max batch: 64, Min lq: 64, Max lq: 256
2025-12-18 10:22:55,825 INFO: [epoch:   6, iter:   4,500, lr:(1.998e-04)] [net: paragonsr2_stream, patch: 256, scale: 2x] [perf: 0.610 it/s, eta: 3 days, 23:08:00] [VRAM: 0.24 GB, peak: 9.74 GB, cached: 6.41 GB] [total: 1.878e-02, content: 1.846e-02] [vram: low] [l_g_l1: 1.8458e-02, l_g_mssim: 3.2393e-04, l_g_total: 1.8782e-02] dynamic_weight_l_g_l1: 9.9995e-01 dynamic_weight_l_g_mssim: 9.9994e-01 grad_norm_g: 7.8530e-03 grad_clip_threshold: 1.0000e+00 scale_g: 1.0000e+00 current_vram_gb: 2.4316e-01 
2025-12-18 10:23:52,180 INFO: User interrupted. Preparing to save state...
2025-12-18 10:23:52,181 INFO: User interrupted. Preparing to save state...
2025-12-18 10:23:52,180 INFO: User interrupted. Preparing to save state...
2025-12-18 10:23:52,180 INFO: User interrupted. Preparing to save state...
2025-12-18 10:23:52,182 INFO: User interrupted. Preparing to save state...
2025-12-18 10:23:52,180 INFO: User interrupted. Preparing to save state...
2025-12-18 10:23:52,181 INFO: User interrupted. Preparing to save state...
2025-12-18 10:23:52,180 INFO: User interrupted. Preparing to save state...
2025-12-18 10:23:52,182 INFO: User interrupted. Preparing to save state...
2025-12-18 10:23:53,379 INFO: Saving models and training states to [link=file:////home/phips/Documents/GitHub/traiNNer-redux/experiments/2xParagonSR2_Stream_fidelity/models]experiments folder[/link] for epoch: 6, iter: 4535.
