#########################################################################################
# ParagonSR2 Nano - CC0 147k Dataset (ZERO-CONFIG AUTOMATED)
#
# This configuration is COMPLETELY AUTOMATIC - no manual parameters needed!
# The framework auto-detects hardware and sets optimal values automatically.
#
# Usage: Just specify dataset paths and architecture - everything else is automatic.
#########################################################################################
#
# ZERO-CONFIG APPROACH (Recommended):
#
# Instead of configuring all these parameters manually, just use:
#
# ```python
# from traiNNer.utils.zero_config_training import create_zero_config_training
#
# config = create_zero_config_training(
#     architecture="paragonsr2_nano",
#     dataset_gt_path="/home/phips/Documents/dataset/cc0/hr",
#     dataset_lq_path="/home/phips/Documents/dataset/cc0/lr_x2_bicubic_aa",
#     val_gt_path="/home/phips/Documents/dataset/cc0/val_hr",
#     val_lq_path="/home/phips/Documents/dataset/cc0/val_lr_x2_bicubic_aa"
# )
#
# # Save generated config
# import yaml
# with open('auto_generated_config.yml', 'w') as f:
#     yaml.dump(config, f)
# ```
#
# The framework automatically detects:
# âœ… Your GPU hardware and VRAM
# âœ… Optimal batch size for your hardware
# âœ… Best automation parameters for your setup
# âœ… AMP/precision settings for your GPU
# âœ… Learning rate and optimization settings
# âœ… All training automation configurations
# âœ… Validation frequency and checkpointing
# âœ… Safety bounds and fallback mechanisms
#
# This gives you the SAME results as manual configuration but with ZERO setup effort!
##########################################################################################

# This file shows what the auto-generated config would look like:
name: 2xParagonSR2_Nano_CC0_147k_AUTO

# Auto-detected system optimizations
use_amp: true              # âœ… Auto-detected from GPU capabilities
amp_bf16: true             # âœ… Auto-selected based on GPU support
use_channels_last: true    # âœ… Auto-enabled for optimal memory layout
fast_matmul: true          # âœ… Auto-enabled for supported hardware
num_gpu: auto              # âœ… Auto-detect available GPUs
manual_seed: 1024

# Dataset configuration (ONLY these paths need to be specified)
datasets:
  train:
    name: CC0_147k_Train
    type: pairedimagedataset
    dataroot_gt: /home/phips/Documents/dataset/cc0/hr          # ğŸ“ USER: Specify this
    dataroot_lq: /home/phips/Documents/dataset/cc0/lr_x2_bicubic_aa  # ğŸ“ USER: Specify this
    lq_size: 128                  # âœ… Auto-optimized for ParagonSR2 Nano
    use_hflip: true
    use_rot: true
    batch_size_per_gpu: 16        # âœ… Auto-calculated from VRAM + architecture
    num_worker_per_gpu: 8         # âœ… Auto-optimized from CPU cores
    accum_iter: 1                 # âœ… Auto-set based on available VRAM
    pin_memory: true              # âœ… Auto-optimized for GPU training

  val:
    name: CC0_147k_Val
    type: pairedimagedataset
    dataroot_gt: /home/phips/Documents/dataset/cc0/val_hr     # ğŸ“ USER: Specify this
    dataroot_lq: /home/phips/Documents/dataset/cc0/val_lr_x2_bicubic_aa  # ğŸ“ USER: Specify this
    lq_size: 128                  # âœ… Auto-matched to training size

network_g:
  type: paragonsr2_nano          # ğŸ“ USER: Specify this

# Complete training configuration (ALL AUTOMATIC)
train:
  ema_decay: 0.999               # âœ… Auto-optimized for stability
  ema_power: 0.75               # âœ… Auto-optimized for convergence
  grad_clip: true               # âœ… Enhanced by adaptive gradient clipping

  optim_g:
    type: AdamW                  # âœ… Auto-selected best optimizer
    lr: !!float 2e-4             # âœ… Auto-optimized for ParagonSR2 Nano
    weight_decay: !!float 1e-4   # âœ… Auto-optimized regularization
    betas: [0.9, 0.99]          # âœ… Auto-optimized momentum

  total_iter: 40000             # âœ… Auto-set from architecture preset
  warmup_iter: 1000             # âœ… Auto-optimized for stable start

  # ğŸ¤– ALL AUTOMATIONS ENABLED - COMPLETELY AUTOMATIC:
  training_automations:
    enabled: true

    # 1. Intelligent Learning Rate Scheduler (Automatic)
    IntelligentLearningRateScheduler:
      enabled: true
      monitor_loss: true
      monitor_validation: true
      adaptation_threshold: 0.02              # âœ… Auto-optimized for stability
      plateau_patience: 1000                  # âœ… Auto-adjusted for hardware tier
      improvement_threshold: 0.001           # âœ… Auto-tuned for sensitivity
      min_lr_factor: 0.1                     # âœ… Safety bound
      max_lr_factor: 2.0                     # âœ… Safety bound
      fallback:
        scheduler_type: "cosine"             # âœ… Auto-selected fallback
        scheduler_params:
          eta_min: 0.00001                   # âœ… Auto-optimized minimum
          T_max: 40000                       # âœ… Matches training duration
      max_adjustments: 50                    # âœ… Safety limit

    # 2. Dynamic Batch Size Optimizer (Automatic)
    DynamicBatchSizeOptimizer:
      enabled: true
      target_vram_usage: 0.85                # âœ… Auto-set from hardware tier
      safety_margin: 0.05                    # âœ… Auto-optimized safety buffer
      adjustment_frequency: 100              # âœ… Auto-balanced for stability
      min_batch_size: 1                      # âœ… Hardware minimum
      max_batch_size: 32                     # âœ… Auto-set from hardware limits
      vram_history_size: 50                  # âœ… Auto-optimized monitoring window
      fallback:
        batch_size: 16                       # âœ… Auto-calculated optimal batch
      max_adjustments: 20                    # âœ… Safety limit

    # 3. Adaptive Gradient Clipping (Automatic)
    AdaptiveGradientClipping:
      enabled: true
      initial_threshold: 1.0                 # âœ… Auto-optimized starting point
      min_threshold: 0.1                     # âœ… Safety bound
      max_threshold: 10.0                    # âœ… Safety bound
      adjustment_factor: 1.2                 # âœ… Auto-tuned adaptation rate
      monitoring_frequency: 10               # âœ… Auto-balanced monitoring
      gradient_history_size: 100             # âœ… Auto-optimized history window
      fallback:
        threshold: 1.0                       # âœ… Safe fallback value
      max_adjustments: 100                   # âœ… Safety limit

    # 4. Intelligent Early Stopping (Automatic)
    IntelligentEarlyStopping:
      enabled: true
      patience: 2000                         # âœ… Auto-adjusted for training length
      min_improvement: 0.001                 # âœ… Auto-tuned sensitivity
      min_epochs: 1000                       # âœ… Auto-set minimum training time
      min_iterations: 5000                   # âœ… Auto-set minimum iterations
      monitor_metric: "val/psnr"             # âœ… Auto-selected best metric
      max_no_improvement: 2000               # âœ… Auto-matched to patience
      improvement_threshold: 0.002           # âœ… Auto-tuned threshold
      warmup_iterations: 1000                # âœ… Auto-matched to warmup
      fallback:
        early_stopping: false                # âœ… Safe fallback
      max_adjustments: 1                     # âœ… One-time decision

  # âœ… Dynamic loss scheduling - completely automatic
  dynamic_loss_scheduling:
    enabled: true
    auto_calibrate: true                     # âœ… Auto-calibrate for architecture

  # âœ… Loss configuration - auto-optimized for architecture
  losses:
    - type: l1loss
      loss_weight: 1.0                       # âœ… Auto-optimized balance
    - type: ssimloss
      loss_weight: 0.05                      # âœ… Auto-optimized balance

# Validation configuration (AUTOMATIC)
val:
  val_enabled: true
  val_freq: 1000                            # âœ… Auto-balanced for training length
  save_img: false

  metrics_enabled: true
  metrics:
    psnr:
      type: calculate_psnr
      crop_border: 4                         # âœ… Auto-selected optimal border
    ssim:
      type: calculate_ssim
      crop_border: 4                         # âœ… Auto-selected optimal border

# Logging configuration (AUTOMATIC)
logger:
  print_freq: 100                           # âœ… Auto-balanced logging
  save_checkpoint_freq: 20000              # âœ… Auto-set for training length
  save_checkpoint_format: safetensors      # âœ… Auto-selected format
  use_tb_logger: true                      # âœ… Auto-enabled for monitoring

# Path configuration (AUTOMATIC)
path:
  pretrain_network_g: ~
  strict_load_g: true
  resume_state: ~


#########################################################################################
# SUMMARY: What the user needs to do vs what the framework does automatically
#########################################################################################
#
# ğŸ“ USER SPECIFIES (3 things only):
# 1. dataset_gt_path: "/path/to/hr/images"
# 2. dataset_lq_path: "/path/to/lr/images"
# 3. architecture: "paragonsr2_nano"
#
# ğŸ¤– FRAMEWORK AUTOMATICALLY DETERMINES (50+ parameters):
# âœ… Hardware detection (GPU, VRAM, CPU, memory)
# âœ… Optimal batch size calculation
# âœ… Automation parameter optimization
# âœ… Safety bounds and thresholds
# âœ… Fallback mechanisms
# âœ… Precision settings (AMP, BF16)
# âœ… Memory layout optimizations
# âœ… Learning rate scheduling
# âœ… Gradient clipping thresholds
# âœ… Early stopping criteria
# âœ… Validation frequency
# âœ… Checkpointing strategy
# âœ… Architecture-specific presets
# âœ… Dataset complexity handling
# âœ… Training duration optimization
# âœ… And much more...
#
# RESULT: Same high-quality training setup with ZERO manual configuration!
##########################################################################################
